{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification, BertModel,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import stats\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD BASELINE RESULTS FROM PART A\n",
    "# =============================================================================\n",
    "\n",
    "def load_baseline_results(results_dir=\"./bias_tracking_results\"):\n",
    "    \"\"\"Load baseline bias propagation results from Part A\"\"\"\n",
    "    \n",
    "    print(\"📋 Loading baseline results from Part A...\")\n",
    "    \n",
    "    trajectory_file = f\"{results_dir}/bias_trajectory.json\"\n",
    "    if not os.path.exists(trajectory_file):\n",
    "        print(f\"❌ Baseline results not found at {results_dir}\")\n",
    "        print(\"Please run Part A first to establish baseline bias propagation.\")\n",
    "        return None\n",
    "    \n",
    "    with open(trajectory_file, 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "    \n",
    "    print(f\"✅ Loaded baseline with {len(baseline_data['bias_trajectory'])} bias measurements\")\n",
    "    print(f\"   Initial SEAT: {baseline_data['initial_bias_state']['seat_effect_size']:.4f}\")\n",
    "    print(f\"   Final SEAT: {baseline_data['final_bias_state']['seat_effect_size']:.4f}\")\n",
    "    \n",
    "    return baseline_data\n",
    "\n",
    "# =============================================================================\n",
    "# FAIRNESS REGULARIZATION IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "class FairnessBertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT with fairness regularization\n",
    "    \n",
    "    Adds bias penalty terms to the loss function to encourage\n",
    "    demographically fair predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=2, fairness_lambda=1.0):\n",
    "        super(FairnessBertForSequenceClassification, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "        # Fairness parameters\n",
    "        self.fairness_lambda = fairness_lambda\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, gender_labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass with fairness regularization\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]  \n",
    "            labels: [batch_size] - sentiment labels\n",
    "            gender_labels: [batch_size] - gender labels (0=male, 1=female, 2=neutral)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Sentiment classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        total_loss = 0\n",
    "        task_loss = None\n",
    "        fairness_loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Standard task loss\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            task_loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            total_loss += task_loss\n",
    "            \n",
    "            # Fairness regularization\n",
    "            if gender_labels is not None and self.fairness_lambda > 0:\n",
    "                fairness_loss = self.calculate_fairness_penalty(\n",
    "                    logits, labels, gender_labels\n",
    "                )\n",
    "                total_loss += self.fairness_lambda * fairness_loss\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'logits': logits,\n",
    "            'task_loss': task_loss,\n",
    "            'fairness_loss': fairness_loss,\n",
    "            'pooled_output': pooled_output\n",
    "        }\n",
    "    \n",
    "    def calculate_fairness_penalty(self, logits, labels, gender_labels):\n",
    "        \"\"\"\n",
    "        Calculate fairness penalty encouraging demographic parity\n",
    "        \n",
    "        Penalty = |P(y=1|male) - P(y=1|female)|^2\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get predictions\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        positive_probs = probs[:, 1]  # Probability of positive sentiment\n",
    "        \n",
    "        # Separate by gender (0=male, 1=female, 2=neutral)\n",
    "        male_mask = (gender_labels == 0)\n",
    "        female_mask = (gender_labels == 1)\n",
    "        \n",
    "        # Calculate group-wise positive prediction rates\n",
    "        if male_mask.sum() > 0 and female_mask.sum() > 0:\n",
    "            male_positive_rate = positive_probs[male_mask].mean()\n",
    "            female_positive_rate = positive_probs[female_mask].mean()\n",
    "            \n",
    "            # Demographic parity penalty\n",
    "            parity_penalty = (male_positive_rate - female_positive_rate) ** 2\n",
    "            \n",
    "            return parity_penalty\n",
    "        else:\n",
    "            return torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_gender_labels(demographic_annotations):\n",
    "    \"\"\"Convert demographic strings to numeric labels\"\"\"\n",
    "    gender_map = {'male': 0, 'female': 1, 'neutral': 2}\n",
    "    return [gender_map[demo] for demo in demographic_annotations]\n",
    "\n",
    "def prepare_fairness_dataset(annotated_dataset, tokenizer, max_length=128):\n",
    "    \"\"\"Prepare dataset with gender labels for fairness training\"\"\"\n",
    "    \n",
    "    def tokenize_and_add_gender(examples):\n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            examples['sentence'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Add gender labels\n",
    "        gender_labels = create_gender_labels(examples['demographic'])\n",
    "        tokenized['gender_labels'] = gender_labels\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    tokenized_dataset = {}\n",
    "    for split_name in ['train', 'validation']:\n",
    "        if split_name in annotated_dataset:\n",
    "            current_columns = annotated_dataset[split_name].column_names\n",
    "            columns_to_remove = [col for col in current_columns \n",
    "                               if col not in ['label', 'demographic']]\n",
    "            \n",
    "            tokenized_dataset[split_name] = annotated_dataset[split_name].map(\n",
    "                tokenize_and_add_gender,\n",
    "                batched=True,\n",
    "                remove_columns=columns_to_remove\n",
    "            )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "class FairnessDataCollator:\n",
    "    \"\"\"Data collator for fairness training\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        batch = {}\n",
    "        batch['input_ids'] = torch.stack([torch.tensor(f['input_ids']) for f in features])\n",
    "        batch['attention_mask'] = torch.stack([torch.tensor(f['attention_mask']) for f in features])\n",
    "        batch['labels'] = torch.tensor([f['label'] for f in features])\n",
    "        batch['gender_labels'] = torch.tensor([f['gender_labels'] for f in features])\n",
    "        return batch\n",
    "\n",
    "# =============================================================================\n",
    "# FAIRNESS TRAINING WITH BIAS TRACKING\n",
    "# =============================================================================\n",
    "\n",
    "def run_fairness_experiment(fairness_lambda, annotated_dataset, seat_examples, \n",
    "                           demographic_test_sets, tokenizer, \n",
    "                           output_dir=\"./fairness_results\"):\n",
    "    \"\"\"\n",
    "    Run single fairness experiment with given lambda value\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔧 Running fairness experiment with λ={fairness_lambda}\")\n",
    "    \n",
    "    # Prepare model\n",
    "    model = FairnessBertForSequenceClassification(\n",
    "        model_name='bert-base-uncased', \n",
    "        num_labels=2, \n",
    "        fairness_lambda=fairness_lambda\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare data\n",
    "    tokenized_dataset = prepare_fairness_dataset(annotated_dataset, tokenizer)\n",
    "    train_dataset = tokenized_dataset['train']\n",
    "    eval_dataset = tokenized_dataset['validation']\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{output_dir}/lambda_{fairness_lambda}\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        eval_steps=300,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_steps=300,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    \n",
    "    # Custom trainer with bias tracking\n",
    "    class FairnessTrainer(Trainer):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            self.bias_measurements = []\n",
    "            super().__init__(*args, **kwargs)\n",
    "        \n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            outputs = model(**inputs)\n",
    "            return (outputs['loss'], outputs) if return_outputs else outputs['loss']\n",
    "        \n",
    "        def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "            # Standard evaluation\n",
    "            eval_results = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "            \n",
    "            # Add bias measurement\n",
    "            bias_state = measure_current_bias_state(\n",
    "                model.bert, tokenizer, seat_examples, demographic_test_sets\n",
    "            )\n",
    "            \n",
    "            # Store bias measurement\n",
    "            self.bias_measurements.append({\n",
    "                'step': self.state.global_step,\n",
    "                'epoch': self.state.epoch,\n",
    "                'eval_accuracy': eval_results.get('eval_accuracy', 0),\n",
    "                'seat_effect_size': bias_state['seat_effect_size'],\n",
    "                'performance_gaps': bias_state['performance_gaps']\n",
    "            })\n",
    "            \n",
    "            return eval_results\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = FairnessTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=FairnessDataCollator(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Measure initial bias\n",
    "    initial_bias = measure_current_bias_state(\n",
    "        model.bert, tokenizer, seat_examples, demographic_test_sets\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"   Training with fairness regularization...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_eval = trainer.evaluate()\n",
    "    final_bias = measure_current_bias_state(\n",
    "        model.bert, tokenizer, seat_examples, demographic_test_sets\n",
    "    )\n",
    "    \n",
    "    # Comprehensive demographic evaluation\n",
    "    final_demographic_performance = evaluate_comprehensive_performance(\n",
    "        model, tokenizer, demographic_test_sets\n",
    "    )\n",
    "    \n",
    "    experiment_results = {\n",
    "        'fairness_lambda': fairness_lambda,\n",
    "        'initial_bias': initial_bias,\n",
    "        'final_bias': final_bias,\n",
    "        'final_accuracy': final_eval['eval_accuracy'],\n",
    "        'bias_measurements': trainer.bias_measurements,\n",
    "        'demographic_performance': final_demographic_performance,\n",
    "        'bias_reduction': initial_bias['seat_effect_size'] - final_bias['seat_effect_size']\n",
    "    }\n",
    "    \n",
    "    print(f\"   Final Accuracy: {final_eval['eval_accuracy']:.4f}\")\n",
    "    print(f\"   Final SEAT Bias: {final_bias['seat_effect_size']:.4f}\")\n",
    "    print(f\"   Bias Reduction: {experiment_results['bias_reduction']:.4f}\")\n",
    "    \n",
    "    return experiment_results\n",
    "\n",
    "# =============================================================================\n",
    "# BIAS MEASUREMENT FUNCTIONS (from Part A)\n",
    "# =============================================================================\n",
    "\n",
    "def get_sentence_embedding(text, model, tokenizer, pooling='cls'):\n",
    "    \"\"\"Extract sentence embedding from BERT model\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, \n",
    "                      truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state[0]\n",
    "    \n",
    "    if pooling == 'cls':\n",
    "        return hidden_states[0].cpu().numpy()\n",
    "    elif pooling == 'mean':\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "        masked_embeddings = hidden_states * attention_mask.unsqueeze(-1)\n",
    "        return (masked_embeddings.sum(dim=0) / attention_mask.sum()).cpu().numpy()\n",
    "\n",
    "def calculate_mini_seat_score(seat_examples, model, tokenizer):\n",
    "    \"\"\"Calculate SEAT bias score\"\"\"\n",
    "    \n",
    "    # Get embeddings for all groups\n",
    "    male_embeddings = [get_sentence_embedding(sent, model, tokenizer) \n",
    "                      for sent in seat_examples['male_targets']]\n",
    "    female_embeddings = [get_sentence_embedding(sent, model, tokenizer) \n",
    "                        for sent in seat_examples['female_targets']]\n",
    "    career_embeddings = [get_sentence_embedding(sent, model, tokenizer) \n",
    "                        for sent in seat_examples['career_attributes']]\n",
    "    family_embeddings = [get_sentence_embedding(sent, model, tokenizer) \n",
    "                        for sent in seat_examples['family_attributes']]\n",
    "    \n",
    "    # Calculate association scores\n",
    "    male_scores = []\n",
    "    female_scores = []\n",
    "    \n",
    "    for male_emb in male_embeddings:\n",
    "        career_sim = np.mean([cosine_similarity([male_emb], [career_emb])[0][0] \n",
    "                             for career_emb in career_embeddings])\n",
    "        family_sim = np.mean([cosine_similarity([male_emb], [family_emb])[0][0] \n",
    "                             for family_emb in family_embeddings])\n",
    "        male_scores.append(career_sim - family_sim)\n",
    "    \n",
    "    for female_emb in female_embeddings:\n",
    "        career_sim = np.mean([cosine_similarity([female_emb], [career_emb])[0][0] \n",
    "                             for career_emb in career_embeddings])\n",
    "        family_sim = np.mean([cosine_similarity([female_emb], [family_emb])[0][0] \n",
    "                             for family_emb in family_embeddings])\n",
    "        female_scores.append(career_sim - family_sim)\n",
    "    \n",
    "    # Calculate effect size\n",
    "    mean_diff = np.mean(male_scores) - np.mean(female_scores)\n",
    "    pooled_std = np.sqrt((np.var(male_scores) + np.var(female_scores)) / 2)\n",
    "    effect_size = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    return effect_size, male_scores, female_scores\n",
    "\n",
    "def evaluate_comprehensive_performance(model, tokenizer, demographic_test_sets):\n",
    "    \"\"\"Comprehensive evaluation across demographic groups\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for demographic, test_data in demographic_test_sets.items():\n",
    "        if len(test_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        confidences = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for example in test_data:\n",
    "                inputs = tokenizer(\n",
    "                    example['sentence'],\n",
    "                    return_tensors='pt',\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                )\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Handle both fairness model and standard BERT\n",
    "                if hasattr(model, 'bert'):\n",
    "                    # Fairness model\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs['logits']\n",
    "                else:\n",
    "                    # Standard BERT\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                \n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                prediction = torch.argmax(logits, dim=-1).item()\n",
    "                confidence = torch.max(probs).item()\n",
    "                \n",
    "                predictions.append(prediction)\n",
    "                true_labels.append(example['label'])\n",
    "                confidences.append(confidence)\n",
    "        \n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        avg_confidence = np.mean(confidences)\n",
    "        \n",
    "        # Calculate positive prediction rate\n",
    "        positive_rate = np.mean([1 for pred in predictions if pred == 1])\n",
    "        \n",
    "        results[demographic] = {\n",
    "            'accuracy': accuracy,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'positive_rate': positive_rate,\n",
    "            'n_examples': len(test_data)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def measure_current_bias_state(model, tokenizer, seat_examples, demographic_test_sets):\n",
    "    \"\"\"Measure current bias state\"\"\"\n",
    "    \n",
    "    # SEAT measurement\n",
    "    seat_effect_size, male_scores, female_scores = calculate_mini_seat_score(\n",
    "        seat_examples, model, tokenizer\n",
    "    )\n",
    "    \n",
    "    # Demographic performance\n",
    "    demographic_results = evaluate_comprehensive_performance(\n",
    "        model, tokenizer, demographic_test_sets\n",
    "    )\n",
    "    \n",
    "    # Performance gaps\n",
    "    performance_gaps = {}\n",
    "    if 'male' in demographic_results and 'female' in demographic_results:\n",
    "        male_acc = demographic_results['male']['accuracy']\n",
    "        female_acc = demographic_results['female']['accuracy']\n",
    "        male_conf = demographic_results['male']['avg_confidence']\n",
    "        female_conf = demographic_results['female']['avg_confidence']\n",
    "        male_pos_rate = demographic_results['male']['positive_rate']\n",
    "        female_pos_rate = demographic_results['female']['positive_rate']\n",
    "        \n",
    "        performance_gaps = {\n",
    "            'accuracy_gap': male_acc - female_acc,\n",
    "            'confidence_gap': male_conf - female_conf,\n",
    "            'positive_rate_gap': male_pos_rate - female_pos_rate,\n",
    "            'male_accuracy': male_acc,\n",
    "            'female_accuracy': female_acc\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'seat_effect_size': seat_effect_size,\n",
    "        'seat_male_scores': male_scores,\n",
    "        'seat_female_scores': female_scores,\n",
    "        'demographic_performance': demographic_results,\n",
    "        'performance_gaps': performance_gaps\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# ACCURACY VS BIAS TRADE-OFF ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def run_trade_off_analysis(baseline_data, output_dir=\"./fairness_analysis\"):\n",
    "    \"\"\"\n",
    "    Run comprehensive accuracy vs bias trade-off analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 STARTING ACCURACY VS BIAS TRADE-OFF ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup from baseline (reuse Part A setup)\n",
    "    print(\"📋 Setting up data and models...\")\n",
    "    \n",
    "    # Import data setup functions from Part A\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Recreate the same setup as Part A\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Load and prepare data (same as Part A)\n",
    "    sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "    male_names, female_names = create_demographic_names()\n",
    "    annotated_dataset = create_demographic_annotated_dataset(sst2_dataset, male_names, female_names)\n",
    "    demographic_test_sets, annotated_dataset = create_demographic_test_sets(annotated_dataset)\n",
    "    seat_examples = create_mini_seat_examples()\n",
    "    \n",
    "    # Define fairness lambda values to test\n",
    "    lambda_values = [0.0, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    \n",
    "    print(f\"🔬 Testing {len(lambda_values)} fairness regularization strengths...\")\n",
    "    print(f\"   Lambda values: {lambda_values}\")\n",
    "    \n",
    "    # Run experiments\n",
    "    all_results = []\n",
    "    \n",
    "    for i, lambda_val in enumerate(lambda_values):\n",
    "        print(f\"\\n📊 Experiment {i+1}/{len(lambda_values)}: λ = {lambda_val}\")\n",
    "        \n",
    "        experiment_result = run_fairness_experiment(\n",
    "            fairness_lambda=lambda_val,\n",
    "            annotated_dataset=annotated_dataset,\n",
    "            seat_examples=seat_examples,\n",
    "            demographic_test_sets=demographic_test_sets,\n",
    "            tokenizer=tokenizer,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        all_results.append(experiment_result)\n",
    "    \n",
    "    # Add baseline to results\n",
    "    baseline_result = {\n",
    "        'fairness_lambda': 'baseline',\n",
    "        'final_accuracy': baseline_data['final_bias_state']['demographic_performance']['all']['accuracy'] if 'all' in baseline_data['final_bias_state']['demographic_performance'] else 0.85,  # fallback\n",
    "        'final_bias': {'seat_effect_size': baseline_data['final_bias_state']['seat_effect_size']},\n",
    "        'bias_reduction': 0  # baseline has no bias reduction\n",
    "    }\n",
    "    all_results.append(baseline_result)\n",
    "    \n",
    "    # Analyze and visualize trade-offs\n",
    "    print(\"\\n📈 Analyzing trade-offs...\")\n",
    "    trade_off_analysis = analyze_accuracy_bias_trade_offs(all_results, baseline_data, output_dir)\n",
    "    \n",
    "    # Save complete results\n",
    "    with open(f\"{output_dir}/trade_off_results.json\", 'w') as f:\n",
    "        json.dump({\n",
    "            'lambda_values': lambda_values,\n",
    "            'all_results': all_results,\n",
    "            'trade_off_analysis': trade_off_analysis,\n",
    "            'baseline_data': baseline_data\n",
    "        }, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n✅ Trade-off analysis complete!\")\n",
    "    print(f\"📁 Results saved to: {output_dir}/\")\n",
    "    \n",
    "    return all_results, trade_off_analysis\n",
    "\n",
    "def analyze_accuracy_bias_trade_offs(all_results, baseline_data, output_dir):\n",
    "    \"\"\"Analyze and visualize accuracy vs bias trade-offs\"\"\"\n",
    "    \n",
    "    # Extract data for analysis\n",
    "    lambda_vals = []\n",
    "    accuracies = []\n",
    "    bias_scores = []\n",
    "    bias_reductions = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result['fairness_lambda'] != 'baseline':\n",
    "            lambda_vals.append(result['fairness_lambda'])\n",
    "            accuracies.append(result['final_accuracy'])\n",
    "            bias_scores.append(result['final_bias']['seat_effect_size'])\n",
    "            bias_reductions.append(result['bias_reduction'])\n",
    "    \n",
    "    # Baseline values\n",
    "    baseline_accuracy = all_results[-1]['final_accuracy']\n",
    "    baseline_bias = all_results[-1]['final_bias']['seat_effect_size']\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Accuracy vs Lambda\n",
    "    ax1.plot(lambda_vals, accuracies, 'bo-', linewidth=2, markersize=8, label='Fairness Regularization')\n",
    "    ax1.axhline(y=baseline_accuracy, color='red', linestyle='--', linewidth=2, label='Baseline (No Regularization)')\n",
    "    ax1.set_xlabel('Fairness Regularization Strength (λ)')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Accuracy vs Fairness Regularization')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    ax1.set_xscale('log')\n",
    "    \n",
    "    # Plot 2: Bias vs Lambda\n",
    "    ax2.plot(lambda_vals, bias_scores, 'ro-', linewidth=2, markersize=8, label='Fairness Regularization')\n",
    "    ax2.axhline(y=baseline_bias, color='red', linestyle='--', linewidth=2, label='Baseline (No Regularization)')\n",
    "    ax2.set_xlabel('Fairness Regularization Strength (λ)')\n",
    "    ax2.set_ylabel('SEAT Effect Size (Bias)')\n",
    "    ax2.set_title('Bias vs Fairness Regularization')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    ax2.set_xscale('log')\n",
    "    \n",
    "    # Plot 3: Trade-off Curve (Accuracy vs Bias)\n",
    "    ax3.scatter(bias_scores, accuracies, c=lambda_vals, s=100, cmap='viridis', alpha=0.8)\n",
    "    ax3.scatter(baseline_bias, baseline_accuracy, c='red', s=150, marker='*', label='Baseline')\n",
    "    \n",
    "    # Add lambda labels\n",
    "    for i, (bias, acc, lam) in enumerate(zip(bias_scores, accuracies, lambda_vals)):\n",
    "        ax3.annotate(f'λ={lam}', (bias, acc), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax3.set_xlabel('SEAT Effect Size (Bias)')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_title('Accuracy vs Bias Trade-off Curve')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(ax3.collections[0], ax=ax3)\n",
    "    cbar.set_label('λ (Fairness Strength)')\n",
    "    \n",
    "    # Plot 4: Bias Reduction vs Accuracy Loss\n",
    "    accuracy_losses = [baseline_accuracy - acc for acc in accuracies]\n",
    "    \n",
    "    ax4.scatter(bias_reductions, accuracy_losses, c=lambda_vals, s=100, cmap='plasma', alpha=0.8)\n",
    "    ax4.set_xlabel('Bias Reduction (SEAT Effect Size)')\n",
    "    ax4.set_ylabel('Accuracy Loss')\n",
    "    ax4.set_title('Bias Reduction vs Accuracy Loss')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add efficiency frontier\n",
    "    if len(bias_reductions) > 1:\n",
    "        # Find Pareto frontier (best trade-offs)\n",
    "        frontier_indices = []\n",
    "        for i, (bias_red, acc_loss) in enumerate(zip(bias_reductions, accuracy_losses)):\n",
    "            is_dominated = False\n",
    "            for j, (other_bias_red, other_acc_loss) in enumerate(zip(bias_reductions, accuracy_losses)):\n",
    "                if i != j and other_bias_red >= bias_red and other_acc_loss <= acc_loss and (other_bias_red > bias_red or other_acc_loss < acc_loss):\n",
    "                    is_dominated = True\n",
    "                    break\n",
    "            if not is_dominated:\n",
    "                frontier_indices.append(i)\n",
    "        \n",
    "        if len(frontier_indices) > 1:\n",
    "            frontier_bias = [bias_reductions[i] for i in frontier_indices]\n",
    "            frontier_acc_loss = [accuracy_losses[i] for i in frontier_indices]\n",
    "            # Sort by bias reduction\n",
    "            sorted_pairs = sorted(zip(frontier_bias, frontier_acc_loss))\n",
    "            frontier_bias, frontier_acc_loss = zip(*sorted_pairs)\n",
    "            ax4.plot(frontier_bias, frontier_acc_loss, 'r--', alpha=0.7, label='Pareto Frontier')\n",
    "            ax4.legend()\n",
    "    \n",
    "    # Add lambda labels\n",
    "    for i, (bias_red, acc_loss, lam) in enumerate(zip(bias_reductions, accuracy_losses, lambda_vals)):\n",
    "        ax4.annotate(f'λ={lam}', (bias_red, acc_loss), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/accuracy_bias_trade_off.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(\"\\n📊 TRADE-OFF ANALYSIS RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Find best trade-offs\n",
    "    best_bias_reduction_idx = np.argmax(bias_reductions)\n",
    "    best_accuracy_idx = np.argmax(accuracies)\n",
    "    \n",
    "    # Calculate efficiency metric (bias reduction per accuracy loss)\n",
    "    efficiency_scores = []\n",
    "    for bias_red, acc_loss in zip(bias_reductions, accuracy_losses):\n",
    "        if acc_loss > 0:\n",
    "            efficiency_scores.append(bias_red / acc_loss)\n",
    "        else:\n",
    "            efficiency_scores.append(float('inf') if bias_red > 0 else 0)\n",
    "    \n",
    "    best_efficiency_idx = np.argmax(efficiency_scores)\n",
    "    \n",
    "    print(f\"🎯 OPTIMAL CONFIGURATIONS:\")\n",
    "    print(f\"   Best Bias Reduction: λ={lambda_vals[best_bias_reduction_idx]:.1f}\")\n",
    "    print(f\"      Bias Reduction: {bias_reductions[best_bias_reduction_idx]:.4f}\")\n",
    "    print(f\"      Accuracy: {accuracies[best_bias_reduction_idx]:.4f}\")\n",
    "    print(f\"      Accuracy Loss: {accuracy_losses[best_bias_reduction_idx]:.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Best Accuracy: λ={lambda_vals[best_accuracy_idx]:.1f}\")\n",
    "    print(f\"      Accuracy: {accuracies[best_accuracy_idx]:.4f}\")\n",
    "    print(f\"      Bias Reduction: {bias_reductions[best_accuracy_idx]:.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Best Efficiency: λ={lambda_vals[best_efficiency_idx]:.1f}\")\n",
    "    print(f\"      Efficiency Score: {efficiency_scores[best_efficiency_idx]:.2f}\")\n",
    "    print(f\"      Bias Reduction: {bias_reductions[best_efficiency_idx]:.4f}\")\n",
    "    print(f\"      Accuracy Loss: {accuracy_losses[best_efficiency_idx]:.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    max_bias_reduction = max(bias_reductions)\n",
    "    min_accuracy_loss = min(accuracy_losses)\n",
    "    \n",
    "    print(f\"\\n📈 SUMMARY STATISTICS:\")\n",
    "    print(f\"   Baseline Bias: {baseline_bias:.4f}\")\n",
    "    print(f\"   Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "    print(f\"   Max Bias Reduction: {max_bias_reduction:.4f}\")\n",
    "    print(f\"   Min Accuracy Loss: {min_accuracy_loss:.4f}\")\n",
    "    print(f\"   Bias Reduction Range: {min(bias_reductions):.4f} to {max_bias_reduction:.4f}\")\n",
    "    print(f\"   Accuracy Range: {min(accuracies):.4f} to {max(accuracies):.4f}\")\n",
    "    \n",
    "    # Calculate correlation between lambda and outcomes\n",
    "    lambda_bias_corr = np.corrcoef(lambda_vals, bias_reductions)[0, 1]\n",
    "    lambda_acc_corr = np.corrcoef(lambda_vals, accuracies)[0, 1]\n",
    "    \n",
    "    print(f\"\\n🔗 CORRELATIONS:\")\n",
    "    print(f\"   λ vs Bias Reduction: {lambda_bias_corr:.3f}\")\n",
    "    print(f\"   λ vs Accuracy: {lambda_acc_corr:.3f}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "    \n",
    "    if max_bias_reduction > 0.5:\n",
    "        print(f\"   ✅ Fairness regularization is EFFECTIVE - achieves substantial bias reduction\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Fairness regularization has LIMITED effectiveness\")\n",
    "    \n",
    "    if min_accuracy_loss < 0.02:\n",
    "        print(f\"   ✅ Bias reduction can be achieved with MINIMAL accuracy loss\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Bias reduction comes with SIGNIFICANT accuracy trade-offs\")\n",
    "    \n",
    "    # Find sweet spot\n",
    "    sweet_spot_idx = best_efficiency_idx\n",
    "    sweet_spot_lambda = lambda_vals[sweet_spot_idx]\n",
    "    \n",
    "    print(f\"   🎯 RECOMMENDED λ = {sweet_spot_lambda:.1f} for best bias-accuracy balance\")\n",
    "    \n",
    "    trade_off_analysis = {\n",
    "        'lambda_values': lambda_vals,\n",
    "        'accuracies': accuracies,\n",
    "        'bias_scores': bias_scores,\n",
    "        'bias_reductions': bias_reductions,\n",
    "        'accuracy_losses': accuracy_losses,\n",
    "        'efficiency_scores': efficiency_scores,\n",
    "        'best_configs': {\n",
    "            'best_bias_reduction': {\n",
    "                'lambda': lambda_vals[best_bias_reduction_idx],\n",
    "                'bias_reduction': bias_reductions[best_bias_reduction_idx],\n",
    "                'accuracy': accuracies[best_bias_reduction_idx]\n",
    "            },\n",
    "            'best_accuracy': {\n",
    "                'lambda': lambda_vals[best_accuracy_idx],\n",
    "                'accuracy': accuracies[best_accuracy_idx],\n",
    "                'bias_reduction': bias_reductions[best_accuracy_idx]\n",
    "            },\n",
    "            'best_efficiency': {\n",
    "                'lambda': lambda_vals[best_efficiency_idx],\n",
    "                'efficiency_score': efficiency_scores[best_efficiency_idx],\n",
    "                'bias_reduction': bias_reductions[best_efficiency_idx],\n",
    "                'accuracy_loss': accuracy_losses[best_efficiency_idx]\n",
    "            }\n",
    "        },\n",
    "        'correlations': {\n",
    "            'lambda_bias_correlation': lambda_bias_corr,\n",
    "            'lambda_accuracy_correlation': lambda_acc_corr\n",
    "        },\n",
    "        'summary_stats': {\n",
    "            'max_bias_reduction': max_bias_reduction,\n",
    "            'min_accuracy_loss': min_accuracy_loss,\n",
    "            'baseline_bias': baseline_bias,\n",
    "            'baseline_accuracy': baseline_accuracy\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return trade_off_analysis\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS (imported from Part A logic)\n",
    "# =============================================================================\n",
    "\n",
    "def create_demographic_names():\n",
    "    \"\"\"Create lists of male and female names for demographic analysis\"\"\"\n",
    "    male_names = [\n",
    "        \"James\", \"John\", \"Robert\", \"Michael\", \"William\", \"David\", \"Richard\", \n",
    "        \"Joseph\", \"Thomas\", \"Christopher\", \"Charles\", \"Daniel\", \"Matthew\", \n",
    "        \"Anthony\", \"Mark\", \"Donald\", \"Steven\", \"Paul\", \"Andrew\", \"Joshua\"\n",
    "    ]\n",
    "    \n",
    "    female_names = [\n",
    "        \"Mary\", \"Patricia\", \"Jennifer\", \"Linda\", \"Elizabeth\", \"Barbara\", \n",
    "        \"Susan\", \"Jessica\", \"Sarah\", \"Karen\", \"Nancy\", \"Lisa\", \"Betty\", \n",
    "        \"Helen\", \"Sandra\", \"Donna\", \"Carol\", \"Ruth\", \"Sharon\", \"Michelle\"\n",
    "    ]\n",
    "    \n",
    "    return male_names, female_names\n",
    "\n",
    "def detect_demographic_mentions(text, male_names, female_names):\n",
    "    \"\"\"Detect if text contains demographic mentions (names or pronouns)\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Check for names\n",
    "    for name in male_names:\n",
    "        if name.lower() in text_lower:\n",
    "            return 'male'\n",
    "    \n",
    "    for name in female_names:\n",
    "        if name.lower() in text_lower:\n",
    "            return 'female'\n",
    "    \n",
    "    # Check for pronouns\n",
    "    male_pronouns = ['he', 'his', 'him', 'himself']\n",
    "    female_pronouns = ['she', 'her', 'hers', 'herself']\n",
    "    \n",
    "    # Use word boundaries to avoid partial matches\n",
    "    for pronoun in male_pronouns:\n",
    "        if re.search(r'\\b' + pronoun + r'\\b', text_lower):\n",
    "            return 'male'\n",
    "    \n",
    "    for pronoun in female_pronouns:\n",
    "        if re.search(r'\\b' + pronoun + r'\\b', text_lower):\n",
    "            return 'female'\n",
    "    \n",
    "    return 'neutral'\n",
    "\n",
    "def create_demographic_annotated_dataset(dataset, male_names, female_names):\n",
    "    \"\"\"Add demographic annotations to SST-2 dataset\"\"\"\n",
    "    \n",
    "    def add_demographic_info(example):\n",
    "        demographic = detect_demographic_mentions(example['sentence'], male_names, female_names)\n",
    "        example['demographic'] = demographic\n",
    "        return example\n",
    "    \n",
    "    # Annotate all splits\n",
    "    annotated_dataset = {}\n",
    "    for split_name in dataset.keys():\n",
    "        annotated_dataset[split_name] = dataset[split_name].map(add_demographic_info)\n",
    "    \n",
    "    return annotated_dataset\n",
    "\n",
    "def create_demographic_test_sets(annotated_dataset, test_split_ratio=0.15):\n",
    "    \"\"\"Create separate test sets for demographic analysis\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Convert training data for splitting\n",
    "    train_data = annotated_dataset['train']\n",
    "    \n",
    "    # Extract data for splitting\n",
    "    sentences = list(train_data['sentence'])\n",
    "    labels = list(train_data['label'])\n",
    "    demographics = list(train_data['demographic'])\n",
    "    \n",
    "    # Stratified split to maintain label distribution\n",
    "    train_sentences, test_sentences, train_labels, test_labels, train_demos, test_demos = train_test_split(\n",
    "        sentences, labels, demographics,\n",
    "        test_size=test_split_ratio,\n",
    "        stratify=labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create new train/test datasets\n",
    "    new_train_data = Dataset.from_dict({\n",
    "        'sentence': train_sentences,\n",
    "        'label': train_labels,\n",
    "        'demographic': train_demos\n",
    "    })\n",
    "    \n",
    "    test_data = Dataset.from_dict({\n",
    "        'sentence': test_sentences,\n",
    "        'label': test_labels,\n",
    "        'demographic': test_demos\n",
    "    })\n",
    "    \n",
    "    # Split test data by demographic groups\n",
    "    male_examples = test_data.filter(lambda x: x['demographic'] == 'male')\n",
    "    female_examples = test_data.filter(lambda x: x['demographic'] == 'female')\n",
    "    neutral_examples = test_data.filter(lambda x: x['demographic'] == 'neutral')\n",
    "    \n",
    "    demographic_splits = {\n",
    "        'male': male_examples,\n",
    "        'female': female_examples,\n",
    "        'neutral': neutral_examples,\n",
    "        'all': test_data\n",
    "    }\n",
    "    \n",
    "    # Update the annotated dataset with new split\n",
    "    annotated_dataset['train'] = new_train_data\n",
    "    annotated_dataset['test'] = test_data\n",
    "    \n",
    "    return demographic_splits, annotated_dataset\n",
    "\n",
    "def create_mini_seat_examples():\n",
    "    \"\"\"Create lightweight SEAT examples for frequent bias measurement\"\"\"\n",
    "    \n",
    "    # Target groups: Male and female names in simple contexts\n",
    "    male_names = [\"James\", \"John\", \"Robert\", \"Michael\"]\n",
    "    female_names = [\"Mary\", \"Patricia\", \"Jennifer\", \"Linda\"]\n",
    "    \n",
    "    male_sentences = [f\"{name} is a person.\" for name in male_names]\n",
    "    female_sentences = [f\"{name} is a person.\" for name in female_names]\n",
    "    \n",
    "    # Attribute groups: Career vs. family oriented (simplified)\n",
    "    career_attributes = [\n",
    "        \"The executive makes decisions.\",\n",
    "        \"The engineer solves problems.\", \n",
    "        \"The manager leads teams.\",\n",
    "        \"The analyst studies data.\"\n",
    "    ]\n",
    "    \n",
    "    family_attributes = [\n",
    "        \"The caregiver helps others.\",\n",
    "        \"The teacher nurtures students.\",\n",
    "        \"The nurse provides care.\",\n",
    "        \"The assistant offers support.\"\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'male_targets': male_sentences,\n",
    "        'female_targets': female_sentences,\n",
    "        'career_attributes': career_attributes,\n",
    "        'family_attributes': family_attributes\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_fairness_analysis(baseline_dir=\"./bias_tracking_results\", \n",
    "                                  output_dir=\"./fairness_analysis\"):\n",
    "    \"\"\"\n",
    "    Run complete fairness regularization analysis\n",
    "    \n",
    "    This function:\n",
    "    1. Loads baseline results from Part A\n",
    "    2. Tests multiple fairness regularization strengths\n",
    "    3. Analyzes accuracy vs bias trade-offs\n",
    "    4. Provides recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 STARTING COMPLETE FAIRNESS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load baseline results\n",
    "    baseline_data = load_baseline_results(baseline_dir)\n",
    "    if baseline_data is None:\n",
    "        return None\n",
    "    \n",
    "    # Run trade-off analysis\n",
    "    all_results, trade_off_analysis = run_trade_off_analysis(baseline_data, output_dir)\n",
    "    \n",
    "    print(\"\\n🎉 FAIRNESS ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"📁 Results saved to: {output_dir}/\")\n",
    "    print(f\"📊 Trade-off plots: {output_dir}/accuracy_bias_trade_off.png\")\n",
    "    print(f\"📋 Full results: {output_dir}/trade_off_results.json\")\n",
    "    \n",
    "    # Quick summary\n",
    "    best_config = trade_off_analysis['best_configs']['best_efficiency']\n",
    "    print(f\"\\n🎯 QUICK SUMMARY:\")\n",
    "    print(f\"   Recommended λ: {best_config['lambda']}\")\n",
    "    print(f\"   Bias Reduction: {best_config['bias_reduction']:.4f}\")\n",
    "    print(f\"   Accuracy Loss: {best_config['accuracy_loss']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'all_results': all_results,\n",
    "        'trade_off_analysis': trade_off_analysis,\n",
    "        'baseline_data': baseline_data\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run complete fairness analysis\n",
    "    print(\"Starting fairness regularization analysis...\")\n",
    "    \n",
    "    results = run_complete_fairness_analysis(\n",
    "        baseline_dir=\"./bias_tracking_results\",\n",
    "        output_dir=\"./fairness_analysis\"\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n✅ Analysis complete!\")\n",
    "        print(\"Check the output directory for detailed results and visualizations.\")\n",
    "    else:\n",
    "        print(\"\\n❌ Analysis failed. Please run Part A first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
