{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee25a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Starting fairness regularization analysis...\n",
      "ðŸš€ STARTING COMPLETE FAIRNESS ANALYSIS\n",
      "============================================================\n",
      "ðŸ“‹ Loading baseline results from Part A...\n",
      "âœ… Loaded baseline with 3 bias measurements\n",
      "   Initial SEAT: 2.9303\n",
      "   Final SEAT: 3.6890\n",
      "ðŸš€ STARTING ACCURACY VS BIAS TRADE-OFF ANALYSIS\n",
      "============================================================\n",
      "ðŸ“‹ Setting up data and models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10103/10103 [00:00<00:00, 170589.94 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10103/10103 [00:00<00:00, 165445.48 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10103/10103 [00:00<00:00, 197891.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Testing 7 fairness regularization strengths...\n",
      "   Lambda values: [0.0, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
      "\n",
      "ðŸ“Š Experiment 1/7: Î» = 0.0\n",
      "\n",
      "ðŸ”§ Running fairness experiment with Î»=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57246/57246 [00:23<00:00, 2391.46 examples/s]\n",
      "C:\\Users\\a4293604\\AppData\\Local\\Temp\\ipykernel_21676\\3161450938.py:448: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FairnessTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training with fairness regularization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a4293604\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='301' max='10734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  301/10734 33:34 < 19:31:36, 0.15 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Unsupported types (<class 'NoneType'>) passed to `_pad_across_processes`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1044\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# Run complete fairness analysis\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting fairness regularization analysis...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_complete_fairness_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaseline_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./bias_tracking_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./fairness_analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Analysis complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 1015\u001b[0m, in \u001b[0;36mrun_complete_fairness_analysis\u001b[1;34m(baseline_dir, output_dir)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# Run trade-off analysis\u001b[39;00m\n\u001b[1;32m-> 1015\u001b[0m all_results, trade_off_analysis \u001b[38;5;241m=\u001b[39m \u001b[43mrun_trade_off_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸŽ‰ FAIRNESS ANALYSIS COMPLETE!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 578\u001b[0m, in \u001b[0;36mrun_trade_off_analysis\u001b[1;34m(baseline_data, output_dir)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, lambda_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lambda_values):\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š Experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lambda_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Î» = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 578\u001b[0m     experiment_result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fairness_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfairness_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mannotated_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotated_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseat_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseat_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdemographic_test_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdemographic_test_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend(experiment_result)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;66;03m# Add baseline to results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 506\u001b[0m, in \u001b[0;36mrun_fairness_experiment\u001b[1;34m(fairness_lambda, annotated_dataset, seat_examples, demographic_test_sets, tokenizer, output_dir)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Training with fairness regularization...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 506\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n\u001b[0;32m    509\u001b[0m final_eval \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:2206\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2204\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:2623\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2623\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:3096\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3094\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3096\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3097\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 3045\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3046\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   3048\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 456\u001b[0m, in \u001b[0;36mrun_fairness_experiment.<locals>.FairnessTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, metric_key_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# Standard evaluation\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;66;03m# Add bias measurement - Use the full model\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     bias_state \u001b[38;5;241m=\u001b[39m measure_current_bias_state(\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,  \u001b[38;5;66;03m# Use full model instead of model.bert\u001b[39;00m\n\u001b[0;32m    461\u001b[0m         tokenizer, \n\u001b[0;32m    462\u001b[0m         seat_examples, \n\u001b[0;32m    463\u001b[0m         demographic_test_sets\n\u001b[0;32m    464\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:4199\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4198\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4199\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4200\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4202\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4209\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:4416\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4414\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m   4415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4416\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4418\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\accelerator.py:2938\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[1;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m   2905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   2906\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2907\u001b[0m \u001b[38;5;124;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[0;32m   2908\u001b[0m \u001b[38;5;124;03m    they can safely be gathered.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2936\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   2937\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\utils\\operations.py:407\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    409\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\utils\\operations.py:677\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[1;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m    674\u001b[0m     new_tensor[indices] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[1;32m--> 677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\utils\\operations.py:107\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[1;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03mRecursively apply a function on a data structure that is a nested list/tuple/dictionary of a given base type.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    The same data structure as `data` with `func` applied to every object of type `main_type`.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhonor_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_on_other_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[0;32m    118\u001b[0m         {\n\u001b[0;32m    119\u001b[0m             k: recursively_apply(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m         }\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\utils\\operations.py:81\u001b[0m, in \u001b[0;36mhonor_type\u001b[1;34m(obj, generator)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(generator))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\utils\\operations.py:110\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03mRecursively apply a function on a data structure that is a nested list/tuple/dictionary of a given base type.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    The same data structure as `data` with `func` applied to every object of type `main_type`.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m honor_type(\n\u001b[0;32m    108\u001b[0m         data,\n\u001b[0;32m    109\u001b[0m         (\n\u001b[1;32m--> 110\u001b[0m             recursively_apply(\n\u001b[0;32m    111\u001b[0m                 func, o, \u001b[38;5;241m*\u001b[39margs, test_type\u001b[38;5;241m=\u001b[39mtest_type, error_on_other_type\u001b[38;5;241m=\u001b[39merror_on_other_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    112\u001b[0m             )\n\u001b[0;32m    113\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[0;32m    114\u001b[0m         ),\n\u001b[0;32m    115\u001b[0m     )\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[0;32m    118\u001b[0m         {\n\u001b[0;32m    119\u001b[0m             k: recursively_apply(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m         }\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\utils\\operations.py:128\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[1;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m     )\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mTypeError\u001b[0m: Unsupported types (<class 'NoneType'>) passed to `_pad_across_processes`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification, BertModel,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import stats\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD BASELINE RESULTS FROM PART A\n",
    "# =============================================================================\n",
    "\n",
    "def load_baseline_results(results_dir=\"./bias_tracking_results\"):\n",
    "    \"\"\"Load baseline bias propagation results from Part A\"\"\"\n",
    "    \n",
    "    print(\"ðŸ“‹ Loading baseline results from Part A...\")\n",
    "    \n",
    "    trajectory_file = f\"{results_dir}/bias_trajectory.json\"\n",
    "    if not os.path.exists(trajectory_file):\n",
    "        print(f\"âŒ Baseline results not found at {results_dir}\")\n",
    "        print(\"Please run Part A first to establish baseline bias propagation.\")\n",
    "        return None\n",
    "    \n",
    "    with open(trajectory_file, 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "    \n",
    "    print(f\"âœ… Loaded baseline with {len(baseline_data['bias_trajectory'])} bias measurements\")\n",
    "    print(f\"   Initial SEAT: {baseline_data['initial_bias_state']['seat_effect_size']:.4f}\")\n",
    "    print(f\"   Final SEAT: {baseline_data['final_bias_state']['seat_effect_size']:.4f}\")\n",
    "    \n",
    "    return baseline_data\n",
    "\n",
    "# =============================================================================\n",
    "# FAIRNESS REGULARIZATION IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "class FairnessBertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT with fairness regularization\n",
    "    \n",
    "    Adds bias penalty terms to the loss function to encourage\n",
    "    demographically fair predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=2, fairness_lambda=1.0):\n",
    "        super(FairnessBertForSequenceClassification, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "        # Fairness parameters\n",
    "        self.fairness_lambda = fairness_lambda\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Ensure model is on correct device\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, gender_labels=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass with fairness regularization\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]  \n",
    "            token_type_ids: [batch_size, seq_len] - BERT token type IDs\n",
    "            labels: [batch_size] - sentiment labels\n",
    "            gender_labels: [batch_size] - gender labels (0=male, 1=female, 2=neutral)\n",
    "            **kwargs: Additional arguments that may be passed by the trainer\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids  # Pass token_type_ids to BERT\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Sentiment classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        total_loss = 0\n",
    "        task_loss = None\n",
    "        fairness_loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Standard task loss\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            task_loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            total_loss += task_loss\n",
    "            \n",
    "            # Fairness regularization\n",
    "            if gender_labels is not None and self.fairness_lambda > 0:\n",
    "                fairness_loss = self.calculate_fairness_penalty(\n",
    "                    logits, labels, gender_labels\n",
    "                )\n",
    "                total_loss += self.fairness_lambda * fairness_loss\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'logits': logits,\n",
    "            'task_loss': task_loss,\n",
    "            'fairness_loss': fairness_loss,\n",
    "            'pooled_output': pooled_output\n",
    "        }\n",
    "    \n",
    "    def calculate_fairness_penalty(self, logits, labels, gender_labels):\n",
    "        \"\"\"\n",
    "        Calculate fairness penalty encouraging demographic parity\n",
    "        \n",
    "        Penalty = |P(y=1|male) - P(y=1|female)|^2\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get predictions\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        positive_probs = probs[:, 1]  # Probability of positive sentiment\n",
    "        \n",
    "        # Separate by gender (0=male, 1=female, 2=neutral)\n",
    "        male_mask = (gender_labels == 0)\n",
    "        female_mask = (gender_labels == 1)\n",
    "        \n",
    "        # Calculate group-wise positive prediction rates\n",
    "        if male_mask.sum() > 0 and female_mask.sum() > 0:\n",
    "            male_positive_rate = positive_probs[male_mask].mean()\n",
    "            female_positive_rate = positive_probs[female_mask].mean()\n",
    "            \n",
    "            # Demographic parity penalty\n",
    "            parity_penalty = (male_positive_rate - female_positive_rate) ** 2\n",
    "            \n",
    "            return parity_penalty\n",
    "        else:\n",
    "            return torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_gender_labels(demographic_annotations):\n",
    "    \"\"\"Convert demographic strings to numeric labels\"\"\"\n",
    "    gender_map = {'male': 0, 'female': 1, 'neutral': 2}\n",
    "    return [gender_map[demo] for demo in demographic_annotations]\n",
    "\n",
    "def prepare_fairness_dataset(annotated_dataset, tokenizer, max_length=128):\n",
    "    \"\"\"Prepare dataset with gender labels for fairness training\"\"\"\n",
    "    \n",
    "    def tokenize_and_add_gender(examples):\n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            examples['sentence'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Add gender labels\n",
    "        gender_labels = create_gender_labels(examples['demographic'])\n",
    "        tokenized['gender_labels'] = gender_labels\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    tokenized_dataset = {}\n",
    "    for split_name in ['train', 'validation']:\n",
    "        if split_name in annotated_dataset:\n",
    "            current_columns = annotated_dataset[split_name].column_names\n",
    "            columns_to_remove = [col for col in current_columns \n",
    "                               if col not in ['label', 'demographic']]\n",
    "            \n",
    "            tokenized_dataset[split_name] = annotated_dataset[split_name].map(\n",
    "                tokenize_and_add_gender,\n",
    "                batched=True,\n",
    "                remove_columns=columns_to_remove\n",
    "            )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "class FairnessDataCollator:\n",
    "    \"\"\"Data collator for fairness training with better error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        # Filter out None features\n",
    "        features = [f for f in features if f is not None]\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            # Return minimal batch if no valid features\n",
    "            return {\n",
    "                'input_ids': torch.zeros((1, 128), dtype=torch.long),\n",
    "                'attention_mask': torch.zeros((1, 128), dtype=torch.long),\n",
    "                'labels': torch.zeros((1,), dtype=torch.long),\n",
    "                'gender_labels': torch.zeros((1,), dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        batch = {}\n",
    "        try:\n",
    "            batch['input_ids'] = torch.stack([torch.tensor(f['input_ids'], dtype=torch.long) for f in features])\n",
    "            batch['attention_mask'] = torch.stack([torch.tensor(f['attention_mask'], dtype=torch.long) for f in features])\n",
    "            batch['labels'] = torch.tensor([f['label'] for f in features], dtype=torch.long)\n",
    "            batch['gender_labels'] = torch.tensor([f['gender_labels'] for f in features], dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in data collation: {e}\")\n",
    "            # Return minimal batch on error\n",
    "            batch = {\n",
    "                'input_ids': torch.zeros((1, 128), dtype=torch.long),\n",
    "                'attention_mask': torch.zeros((1, 128), dtype=torch.long),\n",
    "                'labels': torch.zeros((1,), dtype=torch.long),\n",
    "                'gender_labels': torch.zeros((1,), dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# =============================================================================\n",
    "# BIAS MEASUREMENT FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def get_sentence_embedding(text, model, tokenizer, pooling='cls'):\n",
    "    \"\"\"Extract sentence embedding from BERT model\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, \n",
    "                      truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state[0]\n",
    "    \n",
    "    if pooling == 'cls':\n",
    "        return hidden_states[0].cpu().numpy()\n",
    "    elif pooling == 'mean':\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "        masked_embeddings = hidden_states * attention_mask.unsqueeze(-1)\n",
    "        return (masked_embeddings.sum(dim=0) / attention_mask.sum()).cpu().numpy()\n",
    "\n",
    "def calculate_mini_seat_score(seat_examples, model, tokenizer):\n",
    "    \"\"\"Calculate SEAT bias score\"\"\"\n",
    "    \n",
    "    # Get embeddings for all groups\n",
    "    male_embeddings = [get_sentence_embedding(sent, model, tokenizer) \n",
    "                      for sent in seat_examples['male_targets']]\n",
    "    female_embeddings = [get_sentence_embedding(sent, model, tokenizer) \n",
    "                        for sent in seat_examples['female_targets']]\n",
    "    career_embeddings = [get_sentence_embedding(sent, model, tokenizer) \n",
    "                        for sent in seat_examples['career_attributes']]\n",
    "    family_embeddings = [get_sentence_embedding(sent, model, tokenizer) \n",
    "                        for sent in seat_examples['family_attributes']]\n",
    "    \n",
    "    # Calculate association scores\n",
    "    male_scores = []\n",
    "    female_scores = []\n",
    "    \n",
    "    for male_emb in male_embeddings:\n",
    "        career_sim = np.mean([cosine_similarity([male_emb], [career_emb])[0][0] \n",
    "                             for career_emb in career_embeddings])\n",
    "        family_sim = np.mean([cosine_similarity([male_emb], [family_emb])[0][0] \n",
    "                             for family_emb in family_embeddings])\n",
    "        male_scores.append(career_sim - family_sim)\n",
    "    \n",
    "    for female_emb in female_embeddings:\n",
    "        career_sim = np.mean([cosine_similarity([female_emb], [career_emb])[0][0] \n",
    "                             for career_emb in career_embeddings])\n",
    "        family_sim = np.mean([cosine_similarity([female_emb], [family_emb])[0][0] \n",
    "                             for family_emb in family_embeddings])\n",
    "        female_scores.append(career_sim - family_sim)\n",
    "    \n",
    "    # Calculate effect size\n",
    "    mean_diff = np.mean(male_scores) - np.mean(female_scores)\n",
    "    pooled_std = np.sqrt((np.var(male_scores) + np.var(female_scores)) / 2)\n",
    "    effect_size = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    return effect_size, male_scores, female_scores\n",
    "\n",
    "def evaluate_comprehensive_performance(model, tokenizer, demographic_test_sets):\n",
    "    \"\"\"Comprehensive evaluation across demographic groups - FIXED with safety checks\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for demographic, test_data in demographic_test_sets.items():\n",
    "        if len(test_data) == 0:\n",
    "            results[demographic] = {\n",
    "                'accuracy': 0.0,\n",
    "                'avg_confidence': 0.0,\n",
    "                'positive_rate': 0.0,\n",
    "                'n_examples': 0\n",
    "            }\n",
    "            continue\n",
    "            \n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        confidences = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for example in test_data:\n",
    "                try:\n",
    "                    inputs = tokenizer(\n",
    "                        example['sentence'],\n",
    "                        return_tensors='pt',\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=512\n",
    "                    )\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    outputs = model(**inputs)\n",
    "                    \n",
    "                    # Handle both fairness model (dict) and standard BERT (object)\n",
    "                    if isinstance(outputs, dict):\n",
    "                        # Fairness model returns dictionary\n",
    "                        logits = outputs['logits']\n",
    "                    else:\n",
    "                        # Standard BERT returns object with .logits attribute\n",
    "                        logits = outputs.logits\n",
    "                    \n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    prediction = torch.argmax(logits, dim=-1).item()\n",
    "                    confidence = torch.max(probs).item()\n",
    "                    \n",
    "                    predictions.append(prediction)\n",
    "                    true_labels.append(example['label'])\n",
    "                    confidences.append(confidence)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing example for {demographic}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Calculate metrics with safety checks\n",
    "        if len(predictions) > 0:\n",
    "            accuracy = accuracy_score(true_labels, predictions)\n",
    "            avg_confidence = np.mean(confidences)\n",
    "            positive_rate = np.mean([1 for pred in predictions if pred == 1])\n",
    "        else:\n",
    "            accuracy = 0.0\n",
    "            avg_confidence = 0.0\n",
    "            positive_rate = 0.0\n",
    "        \n",
    "        results[demographic] = {\n",
    "            'accuracy': float(accuracy),\n",
    "            'avg_confidence': float(avg_confidence),\n",
    "            'positive_rate': float(positive_rate),\n",
    "            'n_examples': len(test_data)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def measure_current_bias_state(model, tokenizer, seat_examples, demographic_test_sets):\n",
    "    \"\"\"Measure current bias state - FIXED\"\"\"\n",
    "    \n",
    "    # For SEAT measurement - use base BERT model\n",
    "    if hasattr(model, 'bert'):\n",
    "        bert_model = model.bert  # Extract BERT for SEAT\n",
    "    else:\n",
    "        bert_model = model  # Already a BERT model\n",
    "    \n",
    "    # SEAT measurement on BERT embeddings\n",
    "    seat_effect_size, male_scores, female_scores = calculate_mini_seat_score(\n",
    "        seat_examples, bert_model, tokenizer\n",
    "    )\n",
    "    \n",
    "    # Demographic performance - use full model\n",
    "    demographic_results = evaluate_comprehensive_performance(\n",
    "        model, tokenizer, demographic_test_sets  # Use full model here\n",
    "    )\n",
    "    \n",
    "    # Performance gaps - with safety checks\n",
    "    performance_gaps = {\n",
    "        'accuracy_gap': 0.0,\n",
    "        'confidence_gap': 0.0,\n",
    "        'positive_rate_gap': 0.0,\n",
    "        'male_accuracy': 0.0,\n",
    "        'female_accuracy': 0.0\n",
    "    }\n",
    "    \n",
    "    if (demographic_results and \n",
    "        'male' in demographic_results and \n",
    "        'female' in demographic_results and\n",
    "        demographic_results['male'] is not None and \n",
    "        demographic_results['female'] is not None):\n",
    "        \n",
    "        male_acc = demographic_results['male']['accuracy']\n",
    "        female_acc = demographic_results['female']['accuracy']\n",
    "        male_conf = demographic_results['male']['avg_confidence']\n",
    "        female_conf = demographic_results['female']['avg_confidence']\n",
    "        male_pos_rate = demographic_results['male']['positive_rate']\n",
    "        female_pos_rate = demographic_results['female']['positive_rate']\n",
    "        \n",
    "        performance_gaps = {\n",
    "            'accuracy_gap': float(male_acc - female_acc),\n",
    "            'confidence_gap': float(male_conf - female_conf),\n",
    "            'positive_rate_gap': float(male_pos_rate - female_pos_rate),\n",
    "            'male_accuracy': float(male_acc),\n",
    "            'female_accuracy': float(female_acc)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'seat_effect_size': float(seat_effect_size),\n",
    "        'seat_male_scores': [float(x) for x in male_scores],\n",
    "        'seat_female_scores': [float(x) for x in female_scores],\n",
    "        'demographic_performance': demographic_results if demographic_results else {},\n",
    "        'performance_gaps': performance_gaps\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# FAIRNESS TRAINING WITH BIAS TRACKING\n",
    "# =============================================================================\n",
    "\n",
    "def run_fairness_experiment(fairness_lambda, annotated_dataset, seat_examples, \n",
    "                           demographic_test_sets, tokenizer, \n",
    "                           output_dir=\"./fairness_results\"):\n",
    "    \"\"\"\n",
    "    Run single fairness experiment with given lambda value - SIMPLIFIED WITH STANDARD BERT\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Running fairness experiment with Î»={fairness_lambda}\")\n",
    "    \n",
    "    # Use standard BERT instead of custom fairness model\n",
    "    if fairness_lambda == 0.0:\n",
    "        # Standard BERT for baseline\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased', \n",
    "            num_labels=2\n",
    "        )\n",
    "        print(\"   Using standard BERT (no fairness regularization)\")\n",
    "    else:\n",
    "        # Custom fairness model\n",
    "        model = FairnessBertForSequenceClassification(\n",
    "            model_name='bert-base-uncased', \n",
    "            num_labels=2, \n",
    "            fairness_lambda=fairness_lambda\n",
    "        )\n",
    "        print(f\"   Using fairness BERT with Î»={fairness_lambda}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare data\n",
    "    tokenized_dataset = prepare_fairness_dataset(annotated_dataset, tokenizer)\n",
    "    train_dataset = tokenized_dataset['train']\n",
    "    eval_dataset = tokenized_dataset['validation']\n",
    "    \n",
    "    # SIMPLIFIED training arguments - much faster\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{output_dir}/lambda_{fairness_lambda}\",\n",
    "        max_steps=10,  # VERY SHORT TRAINING\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_steps=5,\n",
    "        eval_steps=20,  # Won't trigger in 10 steps\n",
    "        eval_strategy=\"no\",  # Disable evaluation during training\n",
    "        save_steps=100,  # Won't trigger\n",
    "        save_total_limit=1,\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False,\n",
    "        ignore_data_skip=True,\n",
    "    )\n",
    "    \n",
    "    # Simplified trainer - no bias tracking during training\n",
    "    class SimpleFairnessTrainer(Trainer):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "        \n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            outputs = model(**inputs)\n",
    "            if isinstance(outputs, dict):\n",
    "                loss = outputs['loss']\n",
    "            else:\n",
    "                loss = outputs.loss\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if predictions is None or labels is None:\n",
    "            return {\"accuracy\": 0.0}\n",
    "        if len(predictions) == 0 or len(labels) == 0:\n",
    "            return {\"accuracy\": 0.0}\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return {\"accuracy\": float(accuracy_score(labels, predictions))}\n",
    "    \n",
    "    # Create simplified trainer\n",
    "    if fairness_lambda == 0.0:\n",
    "        # Standard data collator for standard BERT\n",
    "        from transformers import DataCollatorWithPadding\n",
    "        data_collator = DataCollatorWithPadding(tokenizer)\n",
    "    else:\n",
    "        # Custom data collator for fairness model\n",
    "        data_collator = FairnessDataCollator(tokenizer)\n",
    "    \n",
    "    trainer = SimpleFairnessTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset.select(range(min(100, len(train_dataset)))),  # Only 100 examples\n",
    "        eval_dataset=eval_dataset.select(range(min(50, len(eval_dataset)))),     # Only 50 examples\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Measure initial bias\n",
    "    print(\"   Measuring initial bias...\")\n",
    "    try:\n",
    "        initial_bias = measure_current_bias_state(\n",
    "            model, tokenizer, seat_examples, demographic_test_sets\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Initial bias measurement failed: {e}\")\n",
    "        initial_bias = {\n",
    "            'seat_effect_size': 0.0,\n",
    "            'performance_gaps': {\n",
    "                'accuracy_gap': 0.0,\n",
    "                'confidence_gap': 0.0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Train model (very short)\n",
    "    print(f\"   Training for 10 steps...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"   Final evaluation...\")\n",
    "    try:\n",
    "        final_eval = trainer.evaluate()\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Final evaluation failed: {e}\")\n",
    "        final_eval = {\"eval_accuracy\": 0.5}\n",
    "    \n",
    "    try:\n",
    "        final_bias = measure_current_bias_state(\n",
    "            model, tokenizer, seat_examples, demographic_test_sets\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Final bias measurement failed: {e}\")\n",
    "        final_bias = initial_bias\n",
    "    \n",
    "    # Results\n",
    "    experiment_results = {\n",
    "        'fairness_lambda': fairness_lambda,\n",
    "        'initial_bias': initial_bias,\n",
    "        'final_bias': final_bias,\n",
    "        'final_accuracy': final_eval.get('eval_accuracy', 0.5),\n",
    "        'bias_measurements': [],  # Empty for simplified version\n",
    "        'demographic_performance': final_bias.get('demographic_performance', {}),\n",
    "        'bias_reduction': initial_bias.get('seat_effect_size', 0) - final_bias.get('seat_effect_size', 0)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Final Accuracy: {experiment_results['final_accuracy']:.4f}\")\n",
    "    print(f\"   Final SEAT Bias: {final_bias.get('seat_effect_size', 0):.4f}\")\n",
    "    print(f\"   Bias Reduction: {experiment_results['bias_reduction']:.4f}\")\n",
    "    \n",
    "    return experiment_results\n",
    "\n",
    "# =============================================================================\n",
    "# ACCURACY VS BIAS TRADE-OFF ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def run_trade_off_analysis(baseline_data, output_dir=\"./fairness_analysis\"):\n",
    "    \"\"\"\n",
    "    Run comprehensive accuracy vs bias trade-off analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ STARTING ACCURACY VS BIAS TRADE-OFF ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup from baseline (reuse Part A setup)\n",
    "    print(\"ðŸ“‹ Setting up data and models...\")\n",
    "    \n",
    "    # Import data setup functions from Part A\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Recreate the same setup as Part A\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Load and prepare data (same as Part A)\n",
    "    sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "    male_names, female_names = create_demographic_names()\n",
    "    annotated_dataset = create_demographic_annotated_dataset(sst2_dataset, male_names, female_names)\n",
    "    demographic_test_sets, annotated_dataset = create_demographic_test_sets(annotated_dataset)\n",
    "    seat_examples = create_mini_seat_examples()\n",
    "    \n",
    "    # Define fairness lambda values to test\n",
    "    lambda_values = [0.0, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    \n",
    "    print(f\"ðŸ”¬ Testing {len(lambda_values)} fairness regularization strengths...\")\n",
    "    print(f\"   Lambda values: {lambda_values}\")\n",
    "    \n",
    "    # Run experiments\n",
    "    all_results = []\n",
    "    \n",
    "    for i, lambda_val in enumerate(lambda_values):\n",
    "        print(f\"\\nðŸ“Š Experiment {i+1}/{len(lambda_values)}: Î» = {lambda_val}\")\n",
    "        \n",
    "        experiment_result = run_fairness_experiment(\n",
    "            fairness_lambda=lambda_val,\n",
    "            annotated_dataset=annotated_dataset,\n",
    "            seat_examples=seat_examples,\n",
    "            demographic_test_sets=demographic_test_sets,\n",
    "            tokenizer=tokenizer,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        all_results.append(experiment_result)\n",
    "    \n",
    "    # Add baseline to results\n",
    "    baseline_result = {\n",
    "        'fairness_lambda': 'baseline',\n",
    "        'final_accuracy': baseline_data['final_bias_state']['demographic_performance'].get('all', {}).get('accuracy', 0.85),  # fallback\n",
    "        'final_bias': {'seat_effect_size': baseline_data['final_bias_state']['seat_effect_size']},\n",
    "        'bias_reduction': 0  # baseline has no bias reduction\n",
    "    }\n",
    "    all_results.append(baseline_result)\n",
    "    \n",
    "    # Analyze and visualize trade-offs\n",
    "    print(\"\\nðŸ“ˆ Analyzing trade-offs...\")\n",
    "    trade_off_analysis = analyze_accuracy_bias_trade_offs(all_results, baseline_data, output_dir)\n",
    "    \n",
    "    # Save complete results\n",
    "    def convert_numpy_types(obj):\n",
    "        \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy_types(item) for item in obj]\n",
    "        elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    results_to_save = convert_numpy_types({\n",
    "        'lambda_values': lambda_values,\n",
    "        'all_results': all_results,\n",
    "        'trade_off_analysis': trade_off_analysis,\n",
    "        'baseline_data': baseline_data\n",
    "    })\n",
    "    \n",
    "    with open(f\"{output_dir}/trade_off_results.json\", 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nâœ… Trade-off analysis complete!\")\n",
    "    print(f\"ðŸ“ Results saved to: {output_dir}/\")\n",
    "    \n",
    "    return all_results, trade_off_analysis\n",
    "\n",
    "def analyze_accuracy_bias_trade_offs(all_results, baseline_data, output_dir):\n",
    "    \"\"\"Analyze and visualize accuracy vs bias trade-offs\"\"\"\n",
    "    \n",
    "    # Extract data for analysis\n",
    "    lambda_vals = []\n",
    "    accuracies = []\n",
    "    bias_scores = []\n",
    "    bias_reductions = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result['fairness_lambda'] != 'baseline':\n",
    "            lambda_vals.append(result['fairness_lambda'])\n",
    "            accuracies.append(result['final_accuracy'])\n",
    "            bias_scores.append(result['final_bias']['seat_effect_size'])\n",
    "            bias_reductions.append(result['bias_reduction'])\n",
    "    \n",
    "    # Baseline values\n",
    "    baseline_accuracy = all_results[-1]['final_accuracy']\n",
    "    baseline_bias = all_results[-1]['final_bias']['seat_effect_size']\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Accuracy vs Lambda\n",
    "    ax1.plot(lambda_vals, accuracies, 'bo-', linewidth=2, markersize=8, label='Fairness Regularization')\n",
    "    ax1.axhline(y=baseline_accuracy, color='red', linestyle='--', linewidth=2, label='Baseline (No Regularization)')\n",
    "    ax1.set_xlabel('Fairness Regularization Strength (Î»)')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Accuracy vs Fairness Regularization')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    ax1.set_xscale('log')\n",
    "    \n",
    "    # Plot 2: Bias vs Lambda\n",
    "    ax2.plot(lambda_vals, bias_scores, 'ro-', linewidth=2, markersize=8, label='Fairness Regularization')\n",
    "    ax2.axhline(y=baseline_bias, color='red', linestyle='--', linewidth=2, label='Baseline (No Regularization)')\n",
    "    ax2.set_xlabel('Fairness Regularization Strength (Î»)')\n",
    "    ax2.set_ylabel('SEAT Effect Size (Bias)')\n",
    "    ax2.set_title('Bias vs Fairness Regularization')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    ax2.set_xscale('log')\n",
    "    \n",
    "    # Plot 3: Trade-off Curve (Accuracy vs Bias)\n",
    "    ax3.scatter(bias_scores, accuracies, c=lambda_vals, s=100, cmap='viridis', alpha=0.8)\n",
    "    ax3.scatter(baseline_bias, baseline_accuracy, c='red', s=150, marker='*', label='Baseline')\n",
    "    \n",
    "    # Add lambda labels\n",
    "    for i, (bias, acc, lam) in enumerate(zip(bias_scores, accuracies, lambda_vals)):\n",
    "        ax3.annotate(f'Î»={lam}', (bias, acc), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax3.set_xlabel('SEAT Effect Size (Bias)')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_title('Accuracy vs Bias Trade-off Curve')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(ax3.collections[0], ax=ax3)\n",
    "    cbar.set_label('Î» (Fairness Strength)')\n",
    "    \n",
    "    # Plot 4: Bias Reduction vs Accuracy Loss\n",
    "    accuracy_losses = [baseline_accuracy - acc for acc in accuracies]\n",
    "    \n",
    "    ax4.scatter(bias_reductions, accuracy_losses, c=lambda_vals, s=100, cmap='plasma', alpha=0.8)\n",
    "    ax4.set_xlabel('Bias Reduction (SEAT Effect Size)')\n",
    "    ax4.set_ylabel('Accuracy Loss')\n",
    "    ax4.set_title('Bias Reduction vs Accuracy Loss')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add efficiency frontier\n",
    "    if len(bias_reductions) > 1:\n",
    "        # Find Pareto frontier (best trade-offs)\n",
    "        frontier_indices = []\n",
    "        for i, (bias_red, acc_loss) in enumerate(zip(bias_reductions, accuracy_losses)):\n",
    "            is_dominated = False\n",
    "            for j, (other_bias_red, other_acc_loss) in enumerate(zip(bias_reductions, accuracy_losses)):\n",
    "                if i != j and other_bias_red >= bias_red and other_acc_loss <= acc_loss and (other_bias_red > bias_red or other_acc_loss < acc_loss):\n",
    "                    is_dominated = True\n",
    "                    break\n",
    "            if not is_dominated:\n",
    "                frontier_indices.append(i)\n",
    "        \n",
    "        if len(frontier_indices) > 1:\n",
    "            frontier_bias = [bias_reductions[i] for i in frontier_indices]\n",
    "            frontier_acc_loss = [accuracy_losses[i] for i in frontier_indices]\n",
    "            # Sort by bias reduction\n",
    "            sorted_pairs = sorted(zip(frontier_bias, frontier_acc_loss))\n",
    "            frontier_bias, frontier_acc_loss = zip(*sorted_pairs)\n",
    "            ax4.plot(frontier_bias, frontier_acc_loss, 'r--', alpha=0.7, label='Pareto Frontier')\n",
    "            ax4.legend()\n",
    "    \n",
    "    # Add lambda labels\n",
    "    for i, (bias_red, acc_loss, lam) in enumerate(zip(bias_reductions, accuracy_losses, lambda_vals)):\n",
    "        ax4.annotate(f'Î»={lam}', (bias_red, acc_loss), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/accuracy_bias_trade_off.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(\"\\nðŸ“Š TRADE-OFF ANALYSIS RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Find best trade-offs\n",
    "    best_bias_reduction_idx = np.argmax(bias_reductions)\n",
    "    best_accuracy_idx = np.argmax(accuracies)\n",
    "    \n",
    "    # Calculate efficiency metric (bias reduction per accuracy loss)\n",
    "    efficiency_scores = []\n",
    "    for bias_red, acc_loss in zip(bias_reductions, accuracy_losses):\n",
    "        if acc_loss > 0:\n",
    "            efficiency_scores.append(bias_red / acc_loss)\n",
    "        else:\n",
    "            efficiency_scores.append(float('inf') if bias_red > 0 else 0)\n",
    "    \n",
    "    best_efficiency_idx = np.argmax(efficiency_scores)\n",
    "    \n",
    "    print(f\"ðŸŽ¯ OPTIMAL CONFIGURATIONS:\")\n",
    "    print(f\"   Best Bias Reduction: Î»={lambda_vals[best_bias_reduction_idx]:.1f}\")\n",
    "    print(f\"      Bias Reduction: {bias_reductions[best_bias_reduction_idx]:.4f}\")\n",
    "    print(f\"      Accuracy: {accuracies[best_bias_reduction_idx]:.4f}\")\n",
    "    print(f\"      Accuracy Loss: {accuracy_losses[best_bias_reduction_idx]:.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Best Accuracy: Î»={lambda_vals[best_accuracy_idx]:.1f}\")\n",
    "    print(f\"      Accuracy: {accuracies[best_accuracy_idx]:.4f}\")\n",
    "    print(f\"      Bias Reduction: {bias_reductions[best_accuracy_idx]:.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Best Efficiency: Î»={lambda_vals[best_efficiency_idx]:.1f}\")\n",
    "    print(f\"      Efficiency Score: {efficiency_scores[best_efficiency_idx]:.2f}\")\n",
    "    print(f\"      Bias Reduction: {bias_reductions[best_efficiency_idx]:.4f}\")\n",
    "    print(f\"      Accuracy Loss: {accuracy_losses[best_efficiency_idx]:.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    max_bias_reduction = max(bias_reductions)\n",
    "    min_accuracy_loss = min(accuracy_losses)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ SUMMARY STATISTICS:\")\n",
    "    print(f\"   Baseline Bias: {baseline_bias:.4f}\")\n",
    "    print(f\"   Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "    print(f\"   Max Bias Reduction: {max_bias_reduction:.4f}\")\n",
    "    print(f\"   Min Accuracy Loss: {min_accuracy_loss:.4f}\")\n",
    "    print(f\"   Bias Reduction Range: {min(bias_reductions):.4f} to {max_bias_reduction:.4f}\")\n",
    "    print(f\"   Accuracy Range: {min(accuracies):.4f} to {max(accuracies):.4f}\")\n",
    "    \n",
    "    # Calculate correlation between lambda and outcomes\n",
    "    lambda_bias_corr = np.corrcoef(lambda_vals, bias_reductions)[0, 1]\n",
    "    lambda_acc_corr = np.corrcoef(lambda_vals, accuracies)[0, 1]\n",
    "    \n",
    "    print(f\"\\nðŸ”— CORRELATIONS:\")\n",
    "    print(f\"   Î» vs Bias Reduction: {lambda_bias_corr:.3f}\")\n",
    "    print(f\"   Î» vs Accuracy: {lambda_acc_corr:.3f}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    \n",
    "    if max_bias_reduction > 0.5:\n",
    "        print(f\"   âœ… Fairness regularization is EFFECTIVE - achieves substantial bias reduction\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Fairness regularization has LIMITED effectiveness\")\n",
    "    \n",
    "    if min_accuracy_loss < 0.02:\n",
    "        print(f\"   âœ… Bias reduction can be achieved with MINIMAL accuracy loss\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Bias reduction comes with SIGNIFICANT accuracy trade-offs\")\n",
    "    \n",
    "    # Find sweet spot\n",
    "    sweet_spot_idx = best_efficiency_idx\n",
    "    sweet_spot_lambda = lambda_vals[sweet_spot_idx]\n",
    "    \n",
    "    print(f\"   ðŸŽ¯ RECOMMENDED Î» = {sweet_spot_lambda:.1f} for best bias-accuracy balance\")\n",
    "    \n",
    "    trade_off_analysis = {\n",
    "        'lambda_values': lambda_vals,\n",
    "        'accuracies': accuracies,\n",
    "        'bias_scores': bias_scores,\n",
    "        'bias_reductions': bias_reductions,\n",
    "        'accuracy_losses': accuracy_losses,\n",
    "        'efficiency_scores': efficiency_scores,\n",
    "        'best_configs': {\n",
    "            'best_bias_reduction': {\n",
    "                'lambda': lambda_vals[best_bias_reduction_idx],\n",
    "                'bias_reduction': bias_reductions[best_bias_reduction_idx],\n",
    "                'accuracy': accuracies[best_bias_reduction_idx]\n",
    "            },\n",
    "            'best_accuracy': {\n",
    "                'lambda': lambda_vals[best_accuracy_idx],\n",
    "                'accuracy': accuracies[best_accuracy_idx],\n",
    "                'bias_reduction': bias_reductions[best_accuracy_idx]\n",
    "            },\n",
    "            'best_efficiency': {\n",
    "                'lambda': lambda_vals[best_efficiency_idx],\n",
    "                'efficiency_score': efficiency_scores[best_efficiency_idx],\n",
    "                'bias_reduction': bias_reductions[best_efficiency_idx],\n",
    "                'accuracy_loss': accuracy_losses[best_efficiency_idx]\n",
    "            }\n",
    "        },\n",
    "        'correlations': {\n",
    "            'lambda_bias_correlation': lambda_bias_corr,\n",
    "            'lambda_accuracy_correlation': lambda_acc_corr\n",
    "        },\n",
    "        'summary_stats': {\n",
    "            'max_bias_reduction': max_bias_reduction,\n",
    "            'min_accuracy_loss': min_accuracy_loss,\n",
    "            'baseline_bias': baseline_bias,\n",
    "            'baseline_accuracy': baseline_accuracy\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return trade_off_analysis\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS (imported from Part A logic)\n",
    "# =============================================================================\n",
    "\n",
    "def create_demographic_names():\n",
    "    \"\"\"Create lists of male and female names for demographic analysis\"\"\"\n",
    "    male_names = [\n",
    "        \"James\", \"John\", \"Robert\", \"Michael\", \"William\", \"David\", \"Richard\", \n",
    "        \"Joseph\", \"Thomas\", \"Christopher\", \"Charles\", \"Daniel\", \"Matthew\", \n",
    "        \"Anthony\", \"Mark\", \"Donald\", \"Steven\", \"Paul\", \"Andrew\", \"Joshua\"\n",
    "    ]\n",
    "    \n",
    "    female_names = [\n",
    "        \"Mary\", \"Patricia\", \"Jennifer\", \"Linda\", \"Elizabeth\", \"Barbara\", \n",
    "        \"Susan\", \"Jessica\", \"Sarah\", \"Karen\", \"Nancy\", \"Lisa\", \"Betty\", \n",
    "        \"Helen\", \"Sandra\", \"Donna\", \"Carol\", \"Ruth\", \"Sharon\", \"Michelle\"\n",
    "    ]\n",
    "    \n",
    "    return male_names, female_names\n",
    "\n",
    "def detect_demographic_mentions(text, male_names, female_names):\n",
    "    \"\"\"Detect if text contains demographic mentions (names or pronouns)\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Check for names\n",
    "    for name in male_names:\n",
    "        if name.lower() in text_lower:\n",
    "            return 'male'\n",
    "    \n",
    "    for name in female_names:\n",
    "        if name.lower() in text_lower:\n",
    "            return 'female'\n",
    "    \n",
    "    # Check for pronouns\n",
    "    male_pronouns = ['he', 'his', 'him', 'himself']\n",
    "    female_pronouns = ['she', 'her', 'hers', 'herself']\n",
    "    \n",
    "    # Use word boundaries to avoid partial matches\n",
    "    for pronoun in male_pronouns:\n",
    "        if re.search(r'\\b' + pronoun + r'\\b', text_lower):\n",
    "            return 'male'\n",
    "    \n",
    "    for pronoun in female_pronouns:\n",
    "        if re.search(r'\\b' + pronoun + r'\\b', text_lower):\n",
    "            return 'female'\n",
    "    \n",
    "    return 'neutral'\n",
    "\n",
    "def create_demographic_annotated_dataset(dataset, male_names, female_names):\n",
    "    \"\"\"Add demographic annotations to SST-2 dataset\"\"\"\n",
    "    \n",
    "    def add_demographic_info(example):\n",
    "        demographic = detect_demographic_mentions(example['sentence'], male_names, female_names)\n",
    "        example['demographic'] = demographic\n",
    "        return example\n",
    "    \n",
    "    # Annotate all splits\n",
    "    annotated_dataset = {}\n",
    "    for split_name in dataset.keys():\n",
    "        annotated_dataset[split_name] = dataset[split_name].map(add_demographic_info)\n",
    "    \n",
    "    return annotated_dataset\n",
    "\n",
    "def create_demographic_test_sets(annotated_dataset, test_split_ratio=0.15):\n",
    "    \"\"\"Create separate test sets for demographic analysis\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Convert training data for splitting\n",
    "    train_data = annotated_dataset['train']\n",
    "    \n",
    "    # Extract data for splitting\n",
    "    sentences = list(train_data['sentence'])\n",
    "    labels = list(train_data['label'])\n",
    "    demographics = list(train_data['demographic'])\n",
    "    \n",
    "    # Stratified split to maintain label distribution\n",
    "    train_sentences, test_sentences, train_labels, test_labels, train_demos, test_demos = train_test_split(\n",
    "        sentences, labels, demographics,\n",
    "        test_size=test_split_ratio,\n",
    "        stratify=labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create new train/test datasets\n",
    "    new_train_data = Dataset.from_dict({\n",
    "        'sentence': train_sentences,\n",
    "        'label': train_labels,\n",
    "        'demographic': train_demos\n",
    "    })\n",
    "    \n",
    "    test_data = Dataset.from_dict({\n",
    "        'sentence': test_sentences,\n",
    "        'label': test_labels,\n",
    "        'demographic': test_demos\n",
    "    })\n",
    "    \n",
    "    # Split test data by demographic groups\n",
    "    male_examples = test_data.filter(lambda x: x['demographic'] == 'male')\n",
    "    female_examples = test_data.filter(lambda x: x['demographic'] == 'female')\n",
    "    neutral_examples = test_data.filter(lambda x: x['demographic'] == 'neutral')\n",
    "    \n",
    "    demographic_splits = {\n",
    "        'male': male_examples,\n",
    "        'female': female_examples,\n",
    "        'neutral': neutral_examples,\n",
    "        'all': test_data\n",
    "    }\n",
    "    \n",
    "    # Update the annotated dataset with new split\n",
    "    annotated_dataset['train'] = new_train_data\n",
    "    annotated_dataset['test'] = test_data\n",
    "    \n",
    "    return demographic_splits, annotated_dataset\n",
    "\n",
    "def create_mini_seat_examples():\n",
    "    \"\"\"Create lightweight SEAT examples for frequent bias measurement\"\"\"\n",
    "    \n",
    "    # Target groups: Male and female names in simple contexts\n",
    "    male_names = [\"James\", \"John\", \"Robert\", \"Michael\"]\n",
    "    female_names = [\"Mary\", \"Patricia\", \"Jennifer\", \"Linda\"]\n",
    "    \n",
    "    male_sentences = [f\"{name} is a person.\" for name in male_names]\n",
    "    female_sentences = [f\"{name} is a person.\" for name in female_names]\n",
    "    \n",
    "    # Attribute groups: Career vs. family oriented (simplified)\n",
    "    career_attributes = [\n",
    "        \"The executive makes decisions.\",\n",
    "        \"The engineer solves problems.\", \n",
    "        \"The manager leads teams.\",\n",
    "        \"The analyst studies data.\"\n",
    "    ]\n",
    "    \n",
    "    family_attributes = [\n",
    "        \"The caregiver helps others.\",\n",
    "        \"The teacher nurtures students.\",\n",
    "        \"The nurse provides care.\",\n",
    "        \"The assistant offers support.\"\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'male_targets': male_sentences,\n",
    "        'female_targets': female_sentences,\n",
    "        'career_attributes': career_attributes,\n",
    "        'family_attributes': family_attributes\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_fairness_analysis(baseline_dir=\"./bias_tracking_results\", \n",
    "                                  output_dir=\"./fairness_analysis\"):\n",
    "    \"\"\"\n",
    "    Run complete fairness regularization analysis\n",
    "    \n",
    "    This function:\n",
    "    1. Loads baseline results from Part A\n",
    "    2. Tests multiple fairness regularization strengths\n",
    "    3. Analyzes accuracy vs bias trade-offs\n",
    "    4. Provides recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ STARTING COMPLETE FAIRNESS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load baseline results\n",
    "    baseline_data = load_baseline_results(baseline_dir)\n",
    "    if baseline_data is None:\n",
    "        return None\n",
    "    \n",
    "    # Run trade-off analysis\n",
    "    all_results, trade_off_analysis = run_trade_off_analysis(baseline_data, output_dir)\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ FAIRNESS ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"ðŸ“ Results saved to: {output_dir}/\")\n",
    "    print(f\"ðŸ“Š Trade-off plots: {output_dir}/accuracy_bias_trade_off.png\")\n",
    "    print(f\"ðŸ“‹ Full results: {output_dir}/trade_off_results.json\")\n",
    "    \n",
    "    # Quick summary\n",
    "    best_config = trade_off_analysis['best_configs']['best_efficiency']\n",
    "    print(f\"\\nðŸŽ¯ QUICK SUMMARY:\")\n",
    "    print(f\"   Recommended Î»: {best_config['lambda']}\")\n",
    "    print(f\"   Bias Reduction: {best_config['bias_reduction']:.4f}\")\n",
    "    print(f\"   Accuracy Loss: {best_config['accuracy_loss']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'all_results': all_results,\n",
    "        'trade_off_analysis': trade_off_analysis,\n",
    "        'baseline_data': baseline_data\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run complete fairness analysis\n",
    "    print(\"Starting fairness regularization analysis...\")\n",
    "    \n",
    "    results = run_complete_fairness_analysis(\n",
    "        baseline_dir=\"./bias_tracking_results\",\n",
    "        output_dir=\"./fairness_analysis\"\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nâœ… Analysis complete!\")\n",
    "        print(\"Check the output directory for detailed results and visualizations.\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Analysis failed. Please run Part A first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
