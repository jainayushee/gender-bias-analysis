{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670c2271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a4293604\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Download and import datasets\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     28\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç AIF360 Bias Detection with SST-2 Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# AIF360 Bias Detection using SST-2 Dataset and BERT-base-uncased\n",
    "# Detect gender bias in sentiment analysis using real-world movie reviews\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# AIF360 imports\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing, DisparateImpactRemover\n",
    "\n",
    "# Download and import datasets\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîç AIF360 Bias Detection with SST-2 Dataset\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load spaCy for NER (name extraction)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ SpaCy model loaded\")\n",
    "except OSError:\n",
    "    print(\"‚ùå Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Comprehensive name-to-gender mapping\n",
    "GENDER_NAMES = {\n",
    "    'male': {\n",
    "        'james', 'john', 'robert', 'michael', 'william', 'david', 'richard', 'charles', 'joseph', 'thomas',\n",
    "        'daniel', 'matthew', 'anthony', 'mark', 'donald', 'steven', 'paul', 'andrew', 'joshua', 'kenneth',\n",
    "        'kevin', 'brian', 'george', 'edward', 'ronald', 'timothy', 'jason', 'jeffrey', 'ryan', 'jacob',\n",
    "        'gary', 'nicholas', 'eric', 'jonathan', 'stephen', 'larry', 'justin', 'scott', 'brandon', 'benjamin',\n",
    "        'samuel', 'gregory', 'alexander', 'patrick', 'frank', 'raymond', 'jack', 'dennis', 'jerry', 'tyler',\n",
    "        'aaron', 'jose', 'henry', 'adam', 'douglas', 'nathan', 'peter', 'zachary', 'kyle', 'noah',\n",
    "        'alan', 'ethan', 'jeremy', 'lionel', 'mason', 'luke', 'wayne', 'roy', 'eugene', 'louis',\n",
    "        'philip', 'arthur', 'ralph', 'sean', 'austin', 'carl', 'harold', 'roger', 'joe', 'albert'\n",
    "    },\n",
    "    'female': {\n",
    "        'mary', 'patricia', 'jennifer', 'linda', 'elizabeth', 'barbara', 'susan', 'jessica', 'sarah', 'karen',\n",
    "        'nancy', 'lisa', 'betty', 'helen', 'sandra', 'donna', 'carol', 'ruth', 'sharon', 'michelle',\n",
    "        'laura', 'sarah', 'kimberly', 'deborah', 'dorothy', 'lisa', 'nancy', 'karen', 'betty', 'helen',\n",
    "        'sandra', 'donna', 'carol', 'ruth', 'sharon', 'michelle', 'laura', 'emily', 'kimberly', 'deborah',\n",
    "        'dorothy', 'amy', 'angela', 'ashley', 'brenda', 'emma', 'olivia', 'cynthia', 'marie', 'janet',\n",
    "        'catherine', 'frances', 'christine', 'samantha', 'debra', 'rachel', 'carolyn', 'janet', 'virginia',\n",
    "        'maria', 'heather', 'diane', 'julie', 'joyce', 'victoria', 'kelly', 'christina', 'joan', 'evelyn',\n",
    "        'lauren', 'judith', 'megan', 'cheryl', 'andrea', 'hannah', 'jacqueline', 'martha', 'gloria', 'sara'\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_bert_model():\n",
    "    \"\"\"Load BERT-base-uncased model and tokenizer\"\"\"\n",
    "    print(\"üì• Loading BERT-base-uncased...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_sst2_dataset():\n",
    "    \"\"\"Load SST-2 dataset\"\"\"\n",
    "    print(\"üì• Loading SST-2 dataset...\")\n",
    "    dataset = load_dataset(\"glue\", \"sst2\")\n",
    "    \n",
    "    # Combine train and validation for more data\n",
    "    train_data = dataset['train']\n",
    "    val_data = dataset['validation']\n",
    "    \n",
    "    all_sentences = list(train_data['sentence']) + list(val_data['sentence'])\n",
    "    all_labels = list(train_data['label']) + list(val_data['label'])\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(all_sentences)} sentences from SST-2\")\n",
    "    return all_sentences, all_labels\n",
    "\n",
    "def extract_names_from_text(text):\n",
    "    \"\"\"Extract person names from text using SpaCy NER\"\"\"\n",
    "    if nlp is None:\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    names = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            # Clean and normalize name\n",
    "            name = ent.text.lower().strip()\n",
    "            # Remove titles and get first name\n",
    "            name_parts = name.split()\n",
    "            if name_parts:\n",
    "                first_name = name_parts[0].replace(',', '').replace('.', '')\n",
    "                if len(first_name) > 1:  # Avoid single letters\n",
    "                    names.append(first_name)\n",
    "    \n",
    "    return names\n",
    "\n",
    "def infer_gender_from_name(name):\n",
    "    \"\"\"Infer gender from name using predefined lists\"\"\"\n",
    "    name_lower = name.lower()\n",
    "    \n",
    "    if name_lower in GENDER_NAMES['male']:\n",
    "        return 'male'\n",
    "    elif name_lower in GENDER_NAMES['female']:\n",
    "        return 'female'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "def create_gender_annotated_dataset(sentences, labels):\n",
    "    \"\"\"Create dataset with gender annotations from SST-2\"\"\"\n",
    "    print(\"üîÑ Extracting names and inferring gender from SST-2...\")\n",
    "    \n",
    "    annotated_data = []\n",
    "    gender_stats = {'male': 0, 'female': 0, 'unknown': 0, 'no_names': 0}\n",
    "    \n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        names = extract_names_from_text(sentence)\n",
    "        \n",
    "        if not names:\n",
    "            gender_stats['no_names'] += 1\n",
    "            continue\n",
    "        \n",
    "        # For sentences with multiple names, use the first one with known gender\n",
    "        inferred_gender = 'unknown'\n",
    "        used_name = None\n",
    "        \n",
    "        for name in names:\n",
    "            gender = infer_gender_from_name(name)\n",
    "            if gender != 'unknown':\n",
    "                inferred_gender = gender\n",
    "                used_name = name\n",
    "                break\n",
    "        \n",
    "        if inferred_gender != 'unknown':\n",
    "            annotated_data.append({\n",
    "                'sentence': sentence,\n",
    "                'sentiment': label,  # 0=negative, 1=positive\n",
    "                'names': names,\n",
    "                'primary_name': used_name,\n",
    "                'gender': inferred_gender,\n",
    "                'gender_binary': 1 if inferred_gender == 'male' else 0  # 1=male, 0=female\n",
    "            })\n",
    "            gender_stats[inferred_gender] += 1\n",
    "        else:\n",
    "            gender_stats['unknown'] += 1\n",
    "    \n",
    "    print(f\"‚úÖ Gender annotation statistics:\")\n",
    "    for key, value in gender_stats.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    print(f\"üìä Usable samples: {len(annotated_data)}\")\n",
    "    return annotated_data\n",
    "\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    \"\"\"Get BERT embedding for text\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, \n",
    "                          truncation=True, max_length=256)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use CLS token embedding\n",
    "            embedding = outputs.last_hidden_state[0][0]\n",
    "        \n",
    "        return embedding.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_bert_embeddings(data, tokenizer, model, max_samples=2000):\n",
    "    \"\"\"Create BERT embeddings for the dataset\"\"\"\n",
    "    print(f\"üîÑ Creating BERT embeddings (max {max_samples} samples)...\")\n",
    "    \n",
    "    # Balance the dataset\n",
    "    male_samples = [item for item in data if item['gender'] == 'male']\n",
    "    female_samples = [item for item in data if item['gender'] == 'female']\n",
    "    \n",
    "    # Take equal samples from each gender\n",
    "    n_per_gender = min(max_samples // 2, len(male_samples), len(female_samples))\n",
    "    balanced_data = male_samples[:n_per_gender] + female_samples[:n_per_gender]\n",
    "    \n",
    "    print(f\"üìä Using {len(balanced_data)} balanced samples ({n_per_gender} per gender)\")\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for i, item in enumerate(balanced_data):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"   Processing {i}/{len(balanced_data)}...\")\n",
    "        \n",
    "        embedding = get_bert_embedding(item['sentence'], tokenizer, model)\n",
    "        if embedding is not None:\n",
    "            processed_data.append({\n",
    "                'sentence': item['sentence'],\n",
    "                'sentiment': item['sentiment'],\n",
    "                'gender': item['gender'],\n",
    "                'gender_binary': item['gender_binary'],\n",
    "                'primary_name': item['primary_name'],\n",
    "                'embedding': embedding\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(processed_data)} embeddings\")\n",
    "    return processed_data\n",
    "\n",
    "def create_aif360_dataset(data):\n",
    "    \"\"\"Convert data to AIF360 StandardDataset format\"\"\"\n",
    "    print(\"üîß Converting to AIF360 format...\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    embeddings = np.array([item['embedding'] for item in data])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'gender_binary': [item['gender_binary'] for item in data],\n",
    "        'sentiment': [item['sentiment'] for item in data],\n",
    "        'gender': [item['gender'] for item in data],\n",
    "        'sentence': [item['sentence'] for item in data],\n",
    "        'primary_name': [item['primary_name'] for item in data]\n",
    "    })\n",
    "    \n",
    "    # Add embedding dimensions as features\n",
    "    for i in range(embeddings.shape[1]):\n",
    "        df[f'embed_{i}'] = embeddings[:, i]\n",
    "    \n",
    "    # Create AIF360 dataset\n",
    "    aif_dataset = StandardDataset(\n",
    "        df=df,\n",
    "        label_name='sentiment',\n",
    "        favorable_classes=[1],  # positive sentiment is favorable\n",
    "        protected_attribute_names=['gender_binary'],\n",
    "        privileged_classes=[[1]]  # male is privileged class\n",
    "    )\n",
    "    \n",
    "    return aif_dataset, df\n",
    "\n",
    "def compute_bias_metrics(dataset):\n",
    "    \"\"\"Compute comprehensive bias metrics using AIF360\"\"\"\n",
    "    print(\"üìä Computing bias metrics...\")\n",
    "    \n",
    "    metric = BinaryLabelDatasetMetric(\n",
    "        dataset, \n",
    "        unprivileged_groups=[{'gender_binary': 0}],  # female\n",
    "        privileged_groups=[{'gender_binary': 1}]     # male\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'Statistical Parity Difference': metric.statistical_parity_difference(),\n",
    "        'Disparate Impact': metric.disparate_impact(),\n",
    "        'Mean Difference': metric.mean_difference(),\n",
    "        'Base Rate (Female)': metric.base_rate(privileged=False),\n",
    "        'Base Rate (Male)': metric.base_rate(privileged=True),\n",
    "        'Selection Rate (Female)': metric.selection_rate(privileged=False),\n",
    "        'Selection Rate (Male)': metric.selection_rate(privileged=True),\n",
    "        'Smoothed EDF': metric.smoothed_empirical_differential_fairness()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_sentiment_classifier(dataset, df):\n",
    "    \"\"\"Train sentiment classifier and evaluate fairness\"\"\"\n",
    "    print(\"ü§ñ Training sentiment classifier...\")\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    feature_columns = [col for col in df.columns if col.startswith('embed_')]\n",
    "    X = df[feature_columns].values\n",
    "    y = df['sentiment'].values\n",
    "    gender = df['gender_binary'].values\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(\n",
    "        X, y, gender, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Create test dataset for AIF360\n",
    "    test_df = pd.DataFrame({\n",
    "        'gender_binary': gender_test,\n",
    "        'sentiment': y_test,\n",
    "        'prediction': y_pred,\n",
    "        'prediction_proba': y_pred_proba\n",
    "    })\n",
    "    \n",
    "    # Add features\n",
    "    for i, col in enumerate(feature_columns):\n",
    "        test_df[col] = X_test[:, i]\n",
    "    \n",
    "    # Convert to AIF360 format\n",
    "    test_dataset = StandardDataset(\n",
    "        df=test_df,\n",
    "        label_name='sentiment',\n",
    "        favorable_classes=[1],\n",
    "        protected_attribute_names=['gender_binary'],\n",
    "        privileged_classes=[[1]]\n",
    "    )\n",
    "    \n",
    "    # Create prediction dataset\n",
    "    pred_dataset = test_dataset.copy()\n",
    "    pred_dataset.labels = test_df['prediction'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Compute classification metrics\n",
    "    classified_metric = ClassificationMetric(\n",
    "        test_dataset, pred_dataset,\n",
    "        unprivileged_groups=[{'gender_binary': 0}],\n",
    "        privileged_groups=[{'gender_binary': 1}]\n",
    "    )\n",
    "    \n",
    "    classification_metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Equal Opportunity Difference': classified_metric.equal_opportunity_difference(),\n",
    "        'Average Odds Difference': classified_metric.average_odds_difference(),\n",
    "        'Theil Index': classified_metric.theil_index(),\n",
    "        'TPR (Female)': classified_metric.true_positive_rate(privileged=False),\n",
    "        'TPR (Male)': classified_metric.true_positive_rate(privileged=True),\n",
    "        'FPR (Female)': classified_metric.false_positive_rate(privileged=False),\n",
    "        'FPR (Male)': classified_metric.false_positive_rate(privileged=True)\n",
    "    }\n",
    "    \n",
    "    return classification_metrics, test_df, y_test, y_pred\n",
    "\n",
    "def visualize_bias_analysis(bias_metrics, classification_metrics, test_df):\n",
    "    \"\"\"Create comprehensive bias visualization\"\"\"\n",
    "    print(\"üìä Creating bias visualizations...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('BERT Gender Bias Analysis on SST-2 Dataset', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Bias Metrics Bar Chart\n",
    "    metrics_to_plot = ['Statistical Parity Difference', 'Disparate Impact', 'Mean Difference']\n",
    "    values = [bias_metrics[metric] for metric in metrics_to_plot]\n",
    "    colors = ['red' if abs(v) > 0.1 else 'orange' if abs(v) > 0.05 else 'green' for v in values]\n",
    "    \n",
    "    bars = axes[0,0].bar(range(len(metrics_to_plot)), values, color=colors, alpha=0.7)\n",
    "    axes[0,0].set_xlabel('Bias Metrics')\n",
    "    axes[0,0].set_ylabel('Metric Value')\n",
    "    axes[0,0].set_title('AIF360 Bias Metrics')\n",
    "    axes[0,0].set_xticks(range(len(metrics_to_plot)))\n",
    "    axes[0,0].set_xticklabels([m.replace(' ', '\\n') for m in metrics_to_plot], rotation=0)\n",
    "    axes[0,0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, \n",
    "                      bar.get_height() + 0.01 if value > 0 else bar.get_height() - 0.01,\n",
    "                      f'{value:.3f}', ha='center', va='bottom' if value > 0 else 'top')\n",
    "    \n",
    "    # 2. Selection Rates by Gender\n",
    "    male_rate = bias_metrics['Selection Rate (Male)']\n",
    "    female_rate = bias_metrics['Selection Rate (Female)']\n",
    "    \n",
    "    bars = axes[0,1].bar(['Male', 'Female'], [male_rate, female_rate], \n",
    "                        color=['lightblue', 'lightcoral'], alpha=0.7)\n",
    "    axes[0,1].set_ylabel('Positive Sentiment Rate')\n",
    "    axes[0,1].set_title('Selection Rates by Gender')\n",
    "    \n",
    "    for bar, rate in zip(bars, [male_rate, female_rate]):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                      f'{rate:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Classification Performance by Gender\n",
    "    tpr_male = classification_metrics['TPR (Male)']\n",
    "    tpr_female = classification_metrics['TPR (Female)']\n",
    "    fpr_male = classification_metrics['FPR (Male)']\n",
    "    fpr_female = classification_metrics['FPR (Female)']\n",
    "    \n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,2].bar(x - width/2, [tpr_male, tpr_female], width, \n",
    "                 label='True Positive Rate', alpha=0.7, color='green')\n",
    "    axes[0,2].bar(x + width/2, [fpr_male, fpr_female], width,\n",
    "                 label='False Positive Rate', alpha=0.7, color='red')\n",
    "    \n",
    "    axes[0,2].set_xlabel('Gender')\n",
    "    axes[0,2].set_ylabel('Rate')\n",
    "    axes[0,2].set_title('Classification Performance by Gender')\n",
    "    axes[0,2].set_xticks(x)\n",
    "    axes[0,2].set_xticklabels(['Male', 'Female'])\n",
    "    axes[0,2].legend()\n",
    "    \n",
    "    # 4. Gender Distribution in Dataset\n",
    "    gender_counts = test_df['gender_binary'].value_counts()\n",
    "    axes[1,0].pie([gender_counts[0], gender_counts[1]], labels=['Female', 'Male'], \n",
    "                 autopct='%1.1f%%', colors=['lightcoral', 'lightblue'])\n",
    "    axes[1,0].set_title('Gender Distribution in Test Set')\n",
    "    \n",
    "    # 5. Sentiment Distribution by Gender\n",
    "    sentiment_by_gender = test_df.groupby(['gender_binary', 'sentiment']).size().unstack()\n",
    "    sentiment_by_gender.index = ['Female', 'Male']\n",
    "    sentiment_by_gender.columns = ['Negative', 'Positive']\n",
    "    \n",
    "    sentiment_by_gender.plot(kind='bar', ax=axes[1,1], color=['lightcoral', 'lightgreen'])\n",
    "    axes[1,1].set_xlabel('Gender')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    axes[1,1].set_title('Sentiment Distribution by Gender')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # 6. Summary Text\n",
    "    summary_text = f\"\"\"\n",
    "BIAS ANALYSIS SUMMARY\n",
    "\n",
    "Dataset: SST-2 Movie Reviews\n",
    "Model: BERT-base-uncased\n",
    "Samples: {len(test_df)}\n",
    "\n",
    "KEY FINDINGS:\n",
    "Statistical Parity Diff: {bias_metrics['Statistical Parity Difference']:.4f}\n",
    "Disparate Impact: {bias_metrics['Disparate Impact']:.4f}\n",
    "Mean Difference: {bias_metrics['Mean Difference']:.4f}\n",
    "\n",
    "FAIRNESS METRICS:\n",
    "Equal Opportunity Diff: {classification_metrics['Equal Opportunity Difference']:.4f}\n",
    "Average Odds Diff: {classification_metrics['Average Odds Difference']:.4f}\n",
    "Theil Index: {classification_metrics['Theil Index']:.4f}\n",
    "\n",
    "INTERPRETATION:\n",
    "Positive values indicate bias favoring males\n",
    "Negative values indicate bias favoring females\n",
    "Values near 0 indicate fairness\n",
    "\n",
    "BIAS LEVEL: {\"Strong\" if abs(bias_metrics['Statistical Parity Difference']) > 0.1 else \"Moderate\" if abs(bias_metrics['Statistical Parity Difference']) > 0.05 else \"Weak\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1,2].text(0.05, 0.95, summary_text.strip(), transform=axes[1,2].transAxes,\n",
    "                  fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
    "    axes[1,2].set_xlim(0, 1)\n",
    "    axes[1,2].set_ylim(0, 1)\n",
    "    axes[1,2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bert_sst2_bias_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def print_detailed_results(bias_metrics, classification_metrics):\n",
    "    \"\"\"Print detailed bias analysis results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç DETAILED BIAS ANALYSIS RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüìä DATASET-LEVEL BIAS METRICS (AIF360):\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric, value in bias_metrics.items():\n",
    "        interpretation = \"\"\n",
    "        if 'Difference' in metric:\n",
    "            if abs(value) > 0.1:\n",
    "                interpretation = \" (Strong bias)\"\n",
    "            elif abs(value) > 0.05:\n",
    "                interpretation = \" (Moderate bias)\"\n",
    "            else:\n",
    "                interpretation = \" (Weak bias)\"\n",
    "        elif metric == 'Disparate Impact':\n",
    "            if value < 0.8 or value > 1.2:\n",
    "                interpretation = \" (Significant disparity)\"\n",
    "            else:\n",
    "                interpretation = \" (Acceptable)\"\n",
    "        \n",
    "        print(f\"  ‚Ä¢ {metric}: {value:.6f}{interpretation}\")\n",
    "    \n",
    "    print(\"\\nü§ñ CLASSIFICATION-LEVEL BIAS METRICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric, value in classification_metrics.items():\n",
    "        interpretation = \"\"\n",
    "        if 'Difference' in metric:\n",
    "            if abs(value) > 0.1:\n",
    "                interpretation = \" (Unfair)\"\n",
    "            elif abs(value) > 0.05:\n",
    "                interpretation = \" (Moderate unfairness)\"\n",
    "            else:\n",
    "                interpretation = \" (Fair)\"\n",
    "        \n",
    "        print(f\"  ‚Ä¢ {metric}: {value:.6f}{interpretation}\")\n",
    "    \n",
    "    print(\"\\nüéØ BIAS INTERPRETATION:\")\n",
    "    print(\"-\" * 30)\n",
    "    spd = bias_metrics['Statistical Parity Difference']\n",
    "    di = bias_metrics['Disparate Impact']\n",
    "    \n",
    "    if spd > 0.05:\n",
    "        print(\"  üî¥ MALE BIAS DETECTED: Males more likely to get positive sentiment\")\n",
    "    elif spd < -0.05:\n",
    "        print(\"  üî¥ FEMALE BIAS DETECTED: Females more likely to get positive sentiment\")\n",
    "    else:\n",
    "        print(\"  üü¢ MINIMAL BIAS: Sentiment predictions fairly balanced\")\n",
    "    \n",
    "    print(f\"\\nüìà RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 20)\n",
    "    if abs(spd) > 0.1:\n",
    "        print(\"  ‚Ä¢ Apply bias mitigation techniques (Reweighing, Adversarial Debiasing)\")\n",
    "        print(\"  ‚Ä¢ Consider demographic parity constraints\")\n",
    "        print(\"  ‚Ä¢ Audit training data for gender representation\")\n",
    "    elif abs(spd) > 0.05:\n",
    "        print(\"  ‚Ä¢ Monitor bias in production\")\n",
    "        print(\"  ‚Ä¢ Consider light bias mitigation\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Current model shows acceptable fairness\")\n",
    "        print(\"  ‚Ä¢ Continue monitoring in deployment\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"üöÄ Starting BERT SST-2 Bias Analysis...\")\n",
    "    \n",
    "    # Check if spaCy is available\n",
    "    if nlp is None:\n",
    "        print(\"‚ùå SpaCy not available. Please install: python -m spacy download en_core_web_sm\")\n",
    "        return\n",
    "    \n",
    "    # Load models and data\n",
    "    tokenizer, model = load_bert_model()\n",
    "    sentences, labels = load_sst2_dataset()\n",
    "    \n",
    "    # Extract gender information\n",
    "    gender_data = create_gender_annotated_dataset(sentences, labels)\n",
    "    \n",
    "    if len(gender_data) < 100:\n",
    "        print(\"‚ùå Insufficient data with gender information. Need more movie reviews with names.\")\n",
    "        return\n",
    "    \n",
    "    # Create BERT embeddings\n",
    "    processed_data = create_bert_embeddings(gender_data, tokenizer, model)\n",
    "    \n",
    "    # Convert to AIF360 format\n",
    "    aif_dataset, df = create_aif360_dataset(processed_data)\n",
    "    \n",
    "    # Compute bias metrics\n",
    "    bias_metrics = compute_bias_metrics(aif_dataset)\n",
    "    \n",
    "    # Train classifier and evaluate\n",
    "    classification_metrics, test_df, y_test, y_pred = train_sentiment_classifier(aif_dataset, df)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_bias_analysis(bias_metrics, classification_metrics, test_df)\n",
    "    \n",
    "    # Print detailed results\n",
    "    print_detailed_results(bias_metrics, classification_metrics)\n",
    "    \n",
    "    print(\"\\nüéâ BIAS ANALYSIS COMPLETE!\")\n",
    "    print(\"üìÅ Visualization saved as 'bert_sst2_bias_analysis.png'\")\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
